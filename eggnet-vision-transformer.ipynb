{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "\n",
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import math\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import importlib\n",
    "from einops import rearrange, reduce, repeat\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def unfold_output_size(n_times, patches, step):\n",
    "    return (n_times - patches) / step + 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from modules.BciDataHandler import BciDataHandler\n",
    "\n",
    "data_handler = BciDataHandler()\n",
    "data_handler.instantiate_dataset()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_handler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_30556\\1336523557.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# ------------------------------ bci competition dataset ------------------------------\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mall_subject_epochs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmne\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconcatenate_epochs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msubjects_epochs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mall_labels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mall_subject_epochs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mevents\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m-\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mepochs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msubjects_epochs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'data_handler' is not defined"
     ]
    }
   ],
   "source": [
    "# ------------------------------ bci competition dataset ------------------------------\n",
    "all_subject_epochs = mne.concatenate_epochs(list(data_handler.subjects_epochs.values()))\n",
    "all_labels = all_subject_epochs.events[:, -1] - 1\n",
    "\n",
    "epochs = data_handler.subjects_epochs[1]\n",
    "labels = np.array(data_handler.subjects_labels[1]) - 1\n",
    "\n",
    "# epochs = all_subject_epochs\n",
    "# labels = all_labels\n",
    "# labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# -------------------------------- ufjf dataset --------------------------------------\n",
    "# from modules.EdfHandler import EdfHandler\n",
    "\n",
    "# epochs, labels = EdfHandler.getAllData(\n",
    "#     [\n",
    "#         \"C:\\\\Users\\\\davi2\\Desktop\\\\bci\\\\datasets_ufjf\\\\bci\\\\001.edf\",\n",
    "#         \"C:\\\\Users\\\\davi2\\Desktop\\\\bci\\\\datasets_ufjf\\\\bci\\\\002.edf\",\n",
    "#         \"C:\\\\Users\\\\davi2\\Desktop\\\\bci\\\\datasets_ufjf\\\\bci\\\\003.edf\",\n",
    "#         \"C:\\\\Users\\\\davi2\\Desktop\\\\bci\\\\datasets_ufjf\\\\bci\\\\004.edf\",\n",
    "#         \"C:\\\\Users\\\\davi2\\Desktop\\\\bci\\\\datasets_ufjf\\\\bci\\\\005.edf\",\n",
    "#         \"C:\\\\Users\\\\davi2\\Desktop\\\\bci\\\\datasets_ufjf\\\\bci\\\\006.edf\",\n",
    "#     ]\n",
    "# )\n",
    "# epochs = epochs[0]\n",
    "# labels = np.array(labels[0])\n",
    "# labels[labels == 6] = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#----------------------- physionet dataset -------------------------------------\n",
    "# import mne\n",
    "# from mne import Epochs, pick_types, events_from_annotations\n",
    "# from mne.channels import make_standard_montage\n",
    "# from mne.io import concatenate_raws, read_raw_edf\n",
    "# from mne.datasets import eegbci\n",
    "#\n",
    "#\n",
    "# #############################################################################\n",
    "# # Set parameters and read data\n",
    "#\n",
    "# # avoid classification of evoked responses by using epochs that start 1s after\n",
    "# # cue onset.\n",
    "# tmin, tmax = -1., 4.\n",
    "# event_id = dict(handsOrLeft=2, feetOrRight=3)\n",
    "#\n",
    "# def get_physionet_data(subject, runs):\n",
    "#\n",
    "#     raw_fnames = eegbci.load_data(subject, runs)\n",
    "#     raw = concatenate_raws([read_raw_edf(f, preload=True) for f in raw_fnames])\n",
    "#     eegbci.standardize(raw)  # set channel names\n",
    "#     montage = make_standard_montage('standard_1005')\n",
    "#     raw.set_montage(montage)\n",
    "#\n",
    "#     # Apply band-pass filter\n",
    "#     raw.filter(7., 30., fir_design='firwin', skip_by_annotation='edge')\n",
    "#\n",
    "#     events, _ = events_from_annotations(raw)\n",
    "#\n",
    "#     picks = pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False,\n",
    "#                        exclude='bads')\n",
    "#\n",
    "#     # Read epochs (train will be done only between 1 and 2s)\n",
    "#     # Testing will be done with a running classifier\n",
    "#     epochs = Epochs(raw, events, event_id, tmin, tmax, proj=True, picks=picks,\n",
    "#                     baseline=None, preload=True)\n",
    "#\n",
    "#     epochs_data = epochs.copy().crop(tmin=1., tmax=2.)\n",
    "#\n",
    "#     labels = epochs.events[:, -1] - 2\n",
    "#\n",
    "#     return epochs_data, labels\n",
    "#\n",
    "#\n",
    "# # [6, 10, 14] hands vs feet\n",
    "# # [4, 8, 12] left vs right hand\n",
    "# X_hf, y_hf = get_physionet_data(subject=1, runs=[6, 10, 14])\n",
    "# X_lr, y_lr = get_physionet_data(subject=1, runs=[4, 8, 12])\n",
    "#\n",
    "# epochs = mne.concatenate_epochs([X_hf, X_lr])\n",
    "# labels = np.concatenate([y_hf, y_lr+2])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def plot_psd(data, axis, label, color):\n",
    "#     psds, freqs = mne.time_frequency.psd_array_multitaper(data, sfreq=sfreq,\n",
    "#                                                           fmin=0.1, fmax=100)\n",
    "#     psds = 10. * np.log10(psds)\n",
    "#     psds_mean = psds.mean(0).mean(0)\n",
    "#     axis.plot(freqs, psds_mean, color=color, label=label)\n",
    "#\n",
    "#\n",
    "# _, ax = plt.subplots()\n",
    "# plot_psd(X, ax, 'original', 'k')\n",
    "# plot_psd(X_tr.numpy(), ax, 'shifted', 'r')\n",
    "#\n",
    "# ax.set(title='Multitaper PSD (gradiometers)', xlabel='Frequency (Hz)',\n",
    "#        ylabel='Power Spectral Density (dB)')\n",
    "# ax.legend()\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "class MyViTransformerWrapper(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            *,\n",
    "            image_size,\n",
    "            patch_size,\n",
    "            attn_layers,\n",
    "            channels,\n",
    "            num_classes = None,\n",
    "            dropout = 0.,\n",
    "            post_emb_norm = False,\n",
    "            emb_dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert isinstance(attn_layers, Encoder), 'attention layers must be an Encoder'\n",
    "        assert image_size % patch_size == 0, 'image dimensions must be divisible by the patch size'\n",
    "        dim = attn_layers.dim\n",
    "        num_patches = (image_size // patch_size)\n",
    "        patch_dim = channels * patch_size\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(num_patches, dim))\n",
    "\n",
    "        self.patch_to_embedding = nn.Sequential(\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "        self.post_emb_norm = nn.LayerNorm(dim) if post_emb_norm else nn.Identity()\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.attn_layers = attn_layers\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.mlp_head = nn.Linear(dim, num_classes) if exists(num_classes) else nn.Identity()\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            img,\n",
    "            return_embeddings = False\n",
    "    ):\n",
    "        p = self.patch_size\n",
    "        img = img.unsqueeze(-2)\n",
    "        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = 1, p2 = p)\n",
    "        x = self.patch_to_embedding(x)\n",
    "        n = x.shape[1]\n",
    "        x = x + self.pos_embedding\n",
    "\n",
    "        x = self.post_emb_norm(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.attn_layers(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if not exists(self.mlp_head) or return_embeddings:\n",
    "            return x\n",
    "\n",
    "        x = x.mean(dim = -2)\n",
    "        return self.mlp_head(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.utils import initialize_weight\n",
    "from base.layers import Conv2dWithConstraint\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from x_transformers import TransformerWrapper, Encoder, ViTransformerWrapper\n",
    "\n",
    "\n",
    "class FeatureExtraction(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_channels,\n",
    "            kernel_length,\n",
    "            F1,\n",
    "            D,\n",
    "            F2,\n",
    "            pool1_stride,\n",
    "            pool2_stride,\n",
    "            dropout_rate,\n",
    "            weight_init_method=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        pooling_layer = dict(max=nn.MaxPool2d, mean=nn.AvgPool2d)['mean']\n",
    "\n",
    "        if F2 == 'auto':\n",
    "            F2 = F1 * D\n",
    "\n",
    "        # Spectral\n",
    "        self.spectral = nn.Sequential(\n",
    "            Rearrange('b c w -> b 1 c w'),\n",
    "            nn.Conv2d(1, F1, (1, kernel_length), bias=False, padding='same'),\n",
    "            nn.BatchNorm2d(F1)\n",
    "        )\n",
    "\n",
    "        # Spatial\n",
    "        self.spatial = nn.Sequential(\n",
    "            Conv2dWithConstraint(F1, F1 * D, (n_channels, 1), bias=False, groups=F1),\n",
    "            nn.BatchNorm2d(F1 * D),\n",
    "            nn.ELU(),\n",
    "            pooling_layer((1, pool1_stride)),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        # Temporal\n",
    "        self.temporal = nn.Sequential(\n",
    "            nn.Conv2d(F1 * D, F2, (1, n_channels), bias=False, padding='same', groups=F1 * D),\n",
    "            nn.Conv2d(F2, F2, 1, bias=False),\n",
    "            nn.BatchNorm2d(F2),\n",
    "            nn.ELU(),\n",
    "            pooling_layer((1, pool2_stride)),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        initialize_weight(self, weight_init_method)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.spectral(x)\n",
    "        out = self.spatial(out)\n",
    "        out = self.temporal(out)\n",
    "        out = self.classifier(out)\n",
    "        return out\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from base.layers import LinearWithConstraint\n",
    "from einops.layers.torch import Reduce\n",
    "from x_transformers import ContinuousTransformerWrapper\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "class EEGNET(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_times,\n",
    "            n_classes,\n",
    "            n_channels,\n",
    "            patches_size=9,\n",
    "            transformer_dim=32,\n",
    "            transformer_layers=1,\n",
    "            transformer_heads=1,\n",
    "            dropout_rate=0.5,\n",
    "            max_norm=0.25,\n",
    "            F1=8,\n",
    "            F2=16,\n",
    "            D=2,\n",
    "            pool1_stride=4,\n",
    "            pool2_stride=8,\n",
    "            kernel_length=64,\n",
    "            n_hidden=64,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_extraction_output = F2 * ((((n_times - pool1_stride) // pool1_stride + 1) - pool2_stride) // pool2_stride + 1)\n",
    "\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            FeatureExtraction(n_channels=n_channels, kernel_length=kernel_length, F1=F1, D=D, F2=F2, pool1_stride=pool1_stride, pool2_stride=pool2_stride, dropout_rate=dropout_rate),\n",
    "            nn.Linear(in_features=self.feature_extraction_output, out_features=n_hidden),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "\n",
    "        self.transformer = nn.Sequential(\n",
    "            MyViTransformerWrapper(\n",
    "                image_size=n_times,\n",
    "                patch_size=patches_size,\n",
    "                channels=n_channels,\n",
    "                dropout=dropout_rate,\n",
    "                attn_layers=Encoder(\n",
    "                    dim=transformer_dim,\n",
    "                    depth=transformer_layers,\n",
    "                    heads=transformer_heads,\n",
    "                    macaron=True,\n",
    "                    rel_pos_bias=True\n",
    "                ),\n",
    "            ),\n",
    "            nn.Linear(in_features=transformer_dim, out_features=n_hidden),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "\n",
    "        self.spatial_transformer = nn.Sequential(\n",
    "            # Rearrange(\"b c t -> b 1 c t\"),\n",
    "            # nn.Conv2d(in_channels=1, out_channels=4, kernel_size=(8, 32), groups=1),\n",
    "            # nn.Conv2d(in_channels=4, out_channels=1, kernel_size=(8, 32), groups=1),\n",
    "            # nn.AdaptiveAvgPool2d((8, 64)),\n",
    "            # Rearrange(\"b 1 c t -> b t c\"),\n",
    "\n",
    "            nn.AdaptiveAvgPool1d(64), #layer do stop overfit\n",
    "            Rearrange(\"b c t -> b t c\"),\n",
    "            MyViTransformerWrapper(\n",
    "                image_size=n_channels,\n",
    "                patch_size=n_channels//2,\n",
    "                channels=64,\n",
    "                dropout=dropout_rate,\n",
    "                attn_layers=Encoder(\n",
    "                    dim=transformer_dim,\n",
    "                    depth=transformer_layers,\n",
    "                    heads=transformer_heads,\n",
    "                    macaron=True,\n",
    "                    rel_pos_bias=True\n",
    "                ),\n",
    "            ),\n",
    "            # nn.LayerNorm(transformer_dim),\n",
    "            # nn.Dropout(dropout_rate),\n",
    "            nn.Linear(in_features=transformer_dim, out_features=n_hidden),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            LinearWithConstraint(in_features=3*n_hidden, out_features=n_classes, max_norm=max_norm),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        out_values = {}\n",
    "        feature_extraction_result = self.feature_extraction(x)\n",
    "        transformer_result = self.transformer(x)\n",
    "        spatial_transformer_result = self.spatial_transformer(x)\n",
    "        logits = self.head(torch.cat([\n",
    "            feature_extraction_result,\n",
    "            transformer_result,\n",
    "            spatial_transformer_result\n",
    "        ], dim=-1))\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss, out_values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#data augmentation\n",
    "from braindecode.augmentation import FrequencyShift\n",
    "from braindecode.augmentation import GaussianNoise\n",
    "\n",
    "sfreq = epochs.info['sfreq']\n",
    "\n",
    "freq_shift = FrequencyShift(\n",
    "    probability=0.5,  # defines the probability of actually modifying the input\n",
    "    sfreq=sfreq,\n",
    "    max_delta_freq=2.  # the frequency shifts are sampled now between -2 and 2 Hz\n",
    ")\n",
    "\n",
    "gauss_noise = GaussianNoise(\n",
    "    probability=0.5,\n",
    "    std=0.01\n",
    ")\n",
    "\n",
    "transforms = {\n",
    "    'freq': freq_shift,\n",
    "    'gauss': gauss_noise\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = epochs.get_data()\n",
    "X = torch.tensor(data).to(dtype=torch.float32, device=device)\n",
    "y = torch.tensor(labels).to(dtype=torch.long, device=device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = EEGNET(n_times=X.shape[-1], n_channels=len(epochs.picks), n_classes=len(set(labels)))\n",
    "model = model.to(device=device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed = 1330\n",
    "splits = 5\n",
    "lr=3e-4\n",
    "\n",
    "skf = StratifiedKFold(n_splits=splits, random_state=seed, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#fine tune\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from modules.TrainTester import TrainerTester\n",
    "\n",
    "ud = []\n",
    "\n",
    "#main-trianing-loop\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index],\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    TrainerTester.train_loop(model, optimizer, X_train, y_train, X_test, y_test, lr, ud, batch_size=16, iterations=2000)\n",
    "\n",
    "    out_values = TrainerTester.test_loop(model, X_test, y_test)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#params evaluation\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, np.prod(param.size()))\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print('total: ', params)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('model_states/test_model_states.txt'))\n",
    "# torch.save(model.state_dict(), 'model_states/test_model_states.txt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "\n",
    "class TrainerTester:\n",
    "\n",
    "    @staticmethod\n",
    "    def train_loop(model, optimizer, X_train, y_train, X_test, y_test, lr, ud, batch_size=32, iterations=1000):\n",
    "        lossi = []\n",
    "        accuracyi = []\n",
    "        lossv = []\n",
    "        accuracyv = []\n",
    "        kappai = []\n",
    "        kappav = []\n",
    "        for k in range(iterations):\n",
    "            batch_indexes = torch.randint(0, X_train.shape[0], (batch_size,))\n",
    "            batch_test_indexes = torch.randint(0, X_test.shape[0], (batch_size,))\n",
    "\n",
    "            X_batch, y_batch = X_train[batch_indexes], y_train[batch_indexes]  # train batch X,Y\n",
    "\n",
    "            #data augmentation\n",
    "            # X_batch, _ = transforms['freq'].operation(X_batch, None, 10., sfreq)\n",
    "            # X_batch, _ = transforms['gauss'].operation(X_batch, None, std=1e-5)\n",
    "\n",
    "            X_test_batch, y_test_batch = X_test[batch_test_indexes], y_test[batch_test_indexes]  # test batch X,Y\n",
    "\n",
    "            pred, loss, out_values = model(X_batch,  y_batch)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                #tracking training metrics\n",
    "                lossi.append(loss.item())\n",
    "                accuracyi.append((pred.argmax(1) == y_batch).type(torch.float32).sum().item() / y_batch.shape[0])\n",
    "                kappai.append(cohen_kappa_score(pred.argmax(1), y_batch))\n",
    "                #tracking test metrics\n",
    "                test_loss, test_accuracy, test_kappa = TrainerTester.test_loss(model, X_test_batch, y_test_batch)\n",
    "                lossv.append(test_loss)\n",
    "                accuracyv.append(test_accuracy)\n",
    "                kappav.append(test_kappa)\n",
    "\n",
    "                if (k+1) % 100 == 0:\n",
    "                    print(f\"loss: {loss} iteration: {k+1}/{iterations}\")\n",
    "                    plt.title('loss')\n",
    "                    plt.plot(torch.tensor(lossv).view(-1, 10).mean(dim=1).tolist(), label=\"test loss\")\n",
    "                    plt.plot(torch.tensor(lossi).view(-1, 10).mean(dim=1).tolist(), label=\"train loss\")\n",
    "                    plt.show()\n",
    "\n",
    "                    plt.title('accuracy')\n",
    "                    plt.plot(torch.tensor(accuracyv).view(-1, 10).mean(dim=1).tolist(), label=\"test accuracy\")\n",
    "                    plt.plot(torch.tensor(accuracyi).view(-1, 10).mean(dim=1).tolist(), label=\"train accuracy\")\n",
    "                    plt.show()\n",
    "\n",
    "                    plt.title('kappa')\n",
    "                    plt.plot(torch.tensor(kappav).view(-1, 10).mean(dim=1).tolist(), label=\"test accuracy\")\n",
    "                    plt.plot(torch.tensor(kappai).view(-1, 10).mean(dim=1).tolist(), label=\"train accuracy\")\n",
    "                    plt.show()\n",
    "\n",
    "                ud.append([((lr * p.grad).std() / p.data.std()).item() for p in model.parameters()])\n",
    "\n",
    "        return lossi\n",
    "\n",
    "    @staticmethod\n",
    "    def test_loss(model, X_test,  y_test):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred, loss, out_values = model(X_test, y_test)\n",
    "            accuracy = (pred.argmax(1) == y_test).type(torch.float32).sum().item() / y_test.shape[0]\n",
    "            kappa = cohen_kappa_score(pred.argmax(1), y_test)\n",
    "        model.train()\n",
    "        return loss, accuracy, kappa\n",
    "\n",
    "    @staticmethod\n",
    "    def test_loop(model, X_test,  y_test):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred, loss, out_values = model(X_test, y_test)\n",
    "            accuracy = (pred.argmax(1) == y_test).type(torch.float32).sum().item() / y_test.shape[0]\n",
    "            kappa = cohen_kappa_score(pred.argmax(1), y_test)\n",
    "\n",
    "        print(f\"Test loss: {loss:>8f} \\n Accuracy: {accuracy:>8f} \\n kappa: {kappa} \\n\")\n",
    "        model.train()\n",
    "        return out_values\n",
    "\n",
    "    @staticmethod\n",
    "    def test_and_show(model, X_test,  y_test):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred, loss, out_values = model(X_test,  y_test)\n",
    "            accuracy = pred.argmax(1) == y_test\n",
    "            model.train()\n",
    "            return accuracy\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from modules.EdfHandler import EdfHandler\n",
    "\n",
    "epochs_list, labels_list = EdfHandler.getAllData(\n",
    "    [\n",
    "        # \"C:\\\\Users\\\\davi2\\Desktop\\\\bci\\\\datasets_ufjf\\\\bci\\\\001.edf\",\n",
    "        # \"C:\\\\Users\\\\davi2\\Desktop\\\\bci\\\\datasets_ufjf\\\\bci\\\\002.edf\",\n",
    "        # \"C:\\\\Users\\\\davi2\\Desktop\\\\bci\\\\datasets_ufjf\\\\bci\\\\003.edf\",\n",
    "        # \"C:\\\\Users\\\\davi2\\Desktop\\\\bci\\\\datasets_ufjf\\\\bci\\\\004.edf\",\n",
    "        # \"C:\\\\Users\\\\davi2\\Desktop\\\\bci\\\\datasets_ufjf\\\\bci\\\\005.edf\",\n",
    "        \"C:\\\\Users\\\\davi2\\Desktop\\\\bci\\\\datasets_ufjf\\\\bci\\\\006.edf\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "for epochs, labels in zip(epochs_list, labels_list):\n",
    "    labels = np.array(labels)\n",
    "    labels[labels == 6] = 0\n",
    "\n",
    "    data = epochs.get_data()\n",
    "    X = torch.tensor(data).to(dtype=torch.float32, device=device)\n",
    "    y = torch.tensor(labels).to(dtype=torch.long, device=device)\n",
    "\n",
    "    print(X.shape[-1])\n",
    "\n",
    "    model = EEGNET(n_times=X.shape[-1], n_channels=len(epochs.picks), n_classes=len(set(labels)), patches_size=25)\n",
    "    model = model.to(device=device)\n",
    "\n",
    "    seed = 1330\n",
    "    splits = 5\n",
    "    lr=3e-4\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=splits, random_state=seed, shuffle=True)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    ud = []\n",
    "\n",
    "    #main-trianing-loop\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index],\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        TrainerTester.train_loop(model, optimizer, X_train, y_train, X_test, y_test, lr, ud, batch_size=16, iterations=2000)\n",
    "\n",
    "        out_values = TrainerTester.test_loop(model, X_test, y_test)\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
