{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "\n",
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import math\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import importlib\n",
    "from einops import rearrange, reduce, repeat\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from modules.BciDataHandler import BciDataHandler\n",
    "\n",
    "data_handler = BciDataHandler()\n",
    "data_handler.instantiate_dataset()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ------------------------------ bci competition dataset ------------------------------\n",
    "all_subject_epochs = mne.concatenate_epochs(list(data_handler.subjects_epochs.values()))\n",
    "all_labels = all_subject_epochs.events[:, -1] - 1\n",
    "\n",
    "epochs = data_handler.subjects_epochs[2]\n",
    "labels = np.array(data_handler.subjects_labels[2]) - 1\n",
    "\n",
    "\n",
    "# epochs = all_subject_epochs\n",
    "# labels = all_labels\n",
    "# labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "class MyViTransformerWrapper(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            *,\n",
    "            image_size,\n",
    "            patch_size,\n",
    "            attn_layers,\n",
    "            channels,\n",
    "            num_classes = None,\n",
    "            dropout = 0.,\n",
    "            post_emb_norm = False,\n",
    "            emb_dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert isinstance(attn_layers, Encoder), 'attention layers must be an Encoder'\n",
    "        assert image_size % patch_size == 0, 'image dimensions must be divisible by the patch size'\n",
    "        dim = attn_layers.dim\n",
    "        print(\"dim: \", dim)\n",
    "        num_patches = (image_size // patch_size)\n",
    "        patch_dim = channels * patch_size\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(num_patches, dim))\n",
    "\n",
    "        self.patch_to_embedding = nn.Sequential(\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "        self.post_emb_norm = nn.LayerNorm(dim) if post_emb_norm else nn.Identity()\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.attn_layers = attn_layers\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.mlp_head = nn.Linear(dim, num_classes) if exists(num_classes) else nn.Identity()\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            img,\n",
    "            return_embeddings = False\n",
    "    ):\n",
    "        p = self.patch_size\n",
    "        img = img.unsqueeze(-2)\n",
    "        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = 1, p2 = p)\n",
    "        x = self.patch_to_embedding(x)\n",
    "        n = x.shape[1]\n",
    "        x = x + self.pos_embedding\n",
    "\n",
    "        x = self.post_emb_norm(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.attn_layers(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if not exists(self.mlp_head) or return_embeddings:\n",
    "            return x\n",
    "\n",
    "        x = x.mean(dim = -2)\n",
    "        return self.mlp_head(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.utils import initialize_weight\n",
    "from base.layers import Conv2dWithConstraint\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from x_transformers import TransformerWrapper, Encoder, ViTransformerWrapper\n",
    "\n",
    "\n",
    "class FeatureExtraction(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_channels,\n",
    "            kernel_length,\n",
    "            F1,\n",
    "            D,\n",
    "            F2,\n",
    "            pool1_stride,\n",
    "            pool2_stride,\n",
    "            dropout_rate,\n",
    "            weight_init_method=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Spectral\n",
    "        self.rearrange = Rearrange('b c w -> b 1 c w')\n",
    "        self.conv1 = nn.Conv2d(1, F1, (1, kernel_length), bias=False, padding='same')\n",
    "        self.batch1 = nn.BatchNorm2d(F1)\n",
    "\n",
    "        # Spatial\n",
    "        self.conv2 = Conv2dWithConstraint(F1, F1 * D, (n_channels, 1), bias=False, groups=F1)\n",
    "        self.batch2 = nn.BatchNorm2d(F1 * D)\n",
    "        self.activation1 = nn.ELU()\n",
    "        self.avgpool1 = nn.AvgPool2d((1, pool1_stride))\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Temporal\n",
    "        self.conv3 = nn.Conv2d(F1 * D, F2, (1, n_channels), bias=False, padding='same', groups=F2)\n",
    "        self.conv4 = nn.Conv2d(F2, F2, 1, bias=False)\n",
    "        self.batch3 = nn.BatchNorm2d(F2)\n",
    "        self.activation2 = nn.ELU()\n",
    "        self.avgpool2 = nn.AvgPool2d((1, pool2_stride))\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Classifier\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        initialize_weight(self, weight_init_method)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        out = self.rearrange(x)\n",
    "        # print(out.shape)\n",
    "        out = self.conv1(out)\n",
    "        # print(out.shape)\n",
    "        out = self.batch1(out)\n",
    "        # print(out.shape)\n",
    "\n",
    "        # Spatial\n",
    "        out = self.conv2(out)\n",
    "        # print(out.shape)\n",
    "        out = self.batch2(out)\n",
    "        # print(out.shape)\n",
    "        out = self.activation1(out)\n",
    "        # print(out.shape)\n",
    "        out = self.avgpool1(out)\n",
    "        # print(out.shape)\n",
    "        out = self.dropout1(out)\n",
    "        # print(out.shape)\n",
    "\n",
    "        # Temporal\n",
    "        out = self.conv3(out)\n",
    "        # print(out.shape)\n",
    "        out = self.conv4(out)\n",
    "        # print(out.shape)\n",
    "        out = self.batch3(out)\n",
    "        # print(out.shape)\n",
    "        out = self.activation2(out)\n",
    "        # print(out.shape)\n",
    "        out = self.avgpool2(out)\n",
    "        # print(out.shape)\n",
    "        out = self.dropout2(out)\n",
    "        # print(out.shape)\n",
    "        out = self.flatten(out)\n",
    "        # print(out.shape)\n",
    "\n",
    "        return out\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "outputs": [],
   "source": [
    "from base.layers import LinearWithConstraint\n",
    "from einops.layers.torch import Reduce\n",
    "from x_transformers import ContinuousTransformerWrapper\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "class EEGNET(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_times,\n",
    "            n_classes,\n",
    "            n_channels,\n",
    "            patches_size=25,\n",
    "            transformer_dim=4,\n",
    "            transformer_layers=1,\n",
    "            transformer_heads=1,\n",
    "            dropout_rate=0.5,\n",
    "            max_norm=0.25,\n",
    "            F1=8,\n",
    "            F2=16,\n",
    "            D=2,\n",
    "            pool1_stride=4,\n",
    "            pool2_stride=8,\n",
    "            kernel_length=64,\n",
    "            n_hidden=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        kernel_length=128\n",
    "        self.feature_extraction_output = F2 * ((((n_times - pool1_stride) // pool1_stride + 1) - pool2_stride) // pool2_stride + 1)\n",
    "\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            FeatureExtraction(n_channels=n_channels, kernel_length=kernel_length, F1=F1, D=D, F2=F2, pool1_stride=pool1_stride, pool2_stride=pool2_stride, dropout_rate=dropout_rate),\n",
    "            nn.Linear(in_features=self.feature_extraction_output, out_features=n_hidden),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "\n",
    "        self.transformer = nn.Sequential(\n",
    "            MyViTransformerWrapper(\n",
    "                image_size=n_times,\n",
    "                patch_size=patches_size,\n",
    "                channels=n_channels,\n",
    "                dropout=dropout_rate,\n",
    "                attn_layers= Encoder(\n",
    "                    dim=transformer_dim,\n",
    "                    depth=transformer_layers,\n",
    "                    heads=transformer_heads,\n",
    "                    macaron=True,\n",
    "                    rel_pos_bias=True,\n",
    "                    attn_dim_head = 1,\n",
    "                    value_dim_head=1,\n",
    "                    dim_head=1,\n",
    "                    ff_mult=1,\n",
    "                ),\n",
    "            ),\n",
    "            nn.Linear(in_features=transformer_dim, out_features=n_hidden),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "\n",
    "        # self.spatial_transformer = nn.Sequential(\n",
    "        #     # Rearrange(\"b c t -> b 1 c t\"),\n",
    "        #     # nn.Conv2d(in_channels=1, out_channels=4, kernel_size=(8, 32), groups=1),\n",
    "        #     # nn.Conv2d(in_channels=4, out_channels=1, kernel_size=(8, 32), groups=1),\n",
    "        #     # nn.AdaptiveAvgPool2d((8, 64)),\n",
    "        #     # Rearrange(\"b 1 c t -> b t c\"),\n",
    "        #\n",
    "        #     nn.AdaptiveAvgPool1d(128), #layer do stop overfit\n",
    "        #     Rearrange(\"b c t -> b t c\"),\n",
    "        #     MyViTransformerWrapper(\n",
    "        #         image_size=n_channels,\n",
    "        #         patch_size=n_channels//2,\n",
    "        #         channels=128,\n",
    "        #         dropout=dropout_rate,\n",
    "        #         attn_layers=Encoder(\n",
    "        #             dim=transformer_dim,\n",
    "        #             depth=transformer_layers,\n",
    "        #             heads=transformer_heads,\n",
    "        #             macaron=True,\n",
    "        #             rel_pos_bias=True,\n",
    "        #             attn_dim_head = 32,\n",
    "        #         ),\n",
    "        #     ),\n",
    "        #     # nn.LayerNorm(transformer_dim),\n",
    "        #     # nn.Dropout(dropout_rate),\n",
    "        #     nn.Linear(in_features=transformer_dim, out_features=n_hidden),\n",
    "        #     nn.ELU(),\n",
    "        # )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            LinearWithConstraint(in_features=2*n_hidden, out_features=n_classes, max_norm=max_norm),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        out_values = {}\n",
    "        feature_extraction_result = self.feature_extraction(x)\n",
    "        transformer_result = self.transformer(x)\n",
    "        spatial_transformer_result = self.spatial_transformer(x)\n",
    "        logits = self.head(torch.cat([\n",
    "            feature_extraction_result,\n",
    "            transformer_result,\n",
    "            spatial_transformer_result\n",
    "        ], dim=-1))\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss, out_values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "\n",
    "class TrainerTester:\n",
    "\n",
    "    @staticmethod\n",
    "    def train_loop(model, optimizer, X_train, y_train, X_test, y_test, lr, ud, batch_size=32, iterations=1000):\n",
    "        lossi = []\n",
    "        accuracyi = []\n",
    "        lossv = []\n",
    "        accuracyv = []\n",
    "        kappai = []\n",
    "        kappav = []\n",
    "        for k in range(iterations):\n",
    "            batch_indexes = torch.randint(0, X_train.shape[0], (batch_size,))\n",
    "            batch_test_indexes = torch.randint(0, X_test.shape[0], (batch_size,))\n",
    "\n",
    "            X_batch, y_batch = X_train[batch_indexes], y_train[batch_indexes]  # train batch X,Y\n",
    "\n",
    "            #data augmentation\n",
    "            # X_batch, _ = transforms['freq'].operation(X_batch, None, 10., sfreq)\n",
    "            # X_batch, _ = transforms['gauss'].operation(X_batch, None, std=1e-5)\n",
    "\n",
    "            X_test_batch, y_test_batch = X_test[batch_test_indexes], y_test[batch_test_indexes]  # test batch X,Y\n",
    "\n",
    "            pred, loss, out_values = model(X_batch,  y_batch)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                #tracking training metrics\n",
    "                lossi.append(loss.item())\n",
    "                accuracyi.append((pred.argmax(1) == y_batch).type(torch.float32).sum().item() / y_batch.shape[0])\n",
    "                kappai.append(cohen_kappa_score(pred.argmax(1), y_batch))\n",
    "                #tracking test metrics\n",
    "                test_loss, test_accuracy, test_kappa = TrainerTester.test_loss(model, X_test_batch, y_test_batch)\n",
    "                lossv.append(test_loss)\n",
    "                accuracyv.append(test_accuracy)\n",
    "                kappav.append(test_kappa)\n",
    "\n",
    "                if (k+1) % 100 == 0:\n",
    "                    print(f\"loss: {loss} iteration: {k+1}/{iterations}\")\n",
    "                    plt.title('loss')\n",
    "                    plt.plot(torch.tensor(lossv).view(-1, 10).mean(dim=1).tolist(), label=\"test loss\")\n",
    "                    plt.plot(torch.tensor(lossi).view(-1, 10).mean(dim=1).tolist(), label=\"train loss\")\n",
    "                    plt.show()\n",
    "\n",
    "                    plt.title('accuracy')\n",
    "                    plt.plot(torch.tensor(accuracyv).view(-1, 10).mean(dim=1).tolist(), label=\"test accuracy\")\n",
    "                    plt.plot(torch.tensor(accuracyi).view(-1, 10).mean(dim=1).tolist(), label=\"train accuracy\")\n",
    "                    plt.show()\n",
    "\n",
    "                    plt.title('kappa')\n",
    "                    plt.plot(torch.tensor(kappav).view(-1, 10).mean(dim=1).tolist(), label=\"test accuracy\")\n",
    "                    plt.plot(torch.tensor(kappai).view(-1, 10).mean(dim=1).tolist(), label=\"train accuracy\")\n",
    "                    plt.show()\n",
    "\n",
    "                ud.append([((lr * p.grad).std() / p.data.std()).item() for p in model.parameters()])\n",
    "\n",
    "        return lossi\n",
    "\n",
    "    @staticmethod\n",
    "    def test_loss(model, X_test,  y_test):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred, loss, out_values = model(X_test, y_test)\n",
    "            accuracy = (pred.argmax(1) == y_test).type(torch.float32).sum().item() / y_test.shape[0]\n",
    "            kappa = cohen_kappa_score(pred.argmax(1), y_test)\n",
    "        model.train()\n",
    "        return loss, accuracy, kappa\n",
    "\n",
    "    @staticmethod\n",
    "    def test_loop(model, X_test,  y_test):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred, loss, out_values = model(X_test, y_test)\n",
    "            accuracy = (pred.argmax(1) == y_test).type(torch.float32).sum().item() / y_test.shape[0]\n",
    "            kappa = cohen_kappa_score(pred.argmax(1), y_test)\n",
    "\n",
    "        print(f\"Test loss: {loss:>8f} \\n Accuracy: {accuracy:>8f} \\n kappa: {kappa} \\n\")\n",
    "        model.train()\n",
    "        return out_values\n",
    "\n",
    "    @staticmethod\n",
    "    def test_and_show(model, X_test,  y_test):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred, loss, out_values = model(X_test,  y_test)\n",
    "            accuracy = pred.argmax(1) == y_test\n",
    "            model.train()\n",
    "            return accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. , 1.1, 1.2,\n       1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2. , 2.1, 2.2, 2.3, 2.4, 2.5,\n       2.6, 2.7, 2.8, 2.9, 3. , 3.1, 3.2, 3.3, 3.4, 3.5])"
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slide = np.linspace(0, 3.5, 36)\n",
    "slide"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "outputs": [],
   "source": [
    "data = epochs.get_data()\n",
    "X = torch.tensor(data).to(dtype=torch.float32, device=device)\n",
    "y = torch.tensor(labels).to(dtype=torch.long, device=device)\n",
    "X = X[:,:,:-1]\n",
    "\n",
    "#sliding window\n",
    "# slide = np.linspace(0, 3.5, 36)\n",
    "#\n",
    "# x_slides = []\n",
    "# y_slides = []\n",
    "# for i in slide:\n",
    "#     x_slides.append(X[:, :, int(i*250):int((i+1)*250)])\n",
    "#     y_slides.append(y)\n",
    "#\n",
    "#\n",
    "# X = torch.concat(x_slides)\n",
    "# y = torch.concat(y_slides)\n",
    "\n",
    "#downsmaple\n",
    "# X = X[:,:,::2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([576, 22, 1250])"
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim:  8\n"
     ]
    }
   ],
   "source": [
    "model = EEGNET(n_times=X.shape[-1], n_channels=len(epochs.picks), n_classes=len(set(labels)))\n",
    "model = model.to(device=device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_extraction.0.conv1.weight 1024\n",
      "feature_extraction.0.batch1.weight 8\n",
      "feature_extraction.0.batch1.bias 8\n",
      "feature_extraction.0.conv2.weight 352\n",
      "feature_extraction.0.batch2.weight 16\n",
      "feature_extraction.0.batch2.bias 16\n",
      "feature_extraction.0.conv3.weight 352\n",
      "feature_extraction.0.conv4.weight 256\n",
      "feature_extraction.0.batch3.weight 16\n",
      "feature_extraction.0.batch3.bias 16\n",
      "feature_extraction.1.weight 2496\n",
      "feature_extraction.1.bias 4\n",
      "transformer.0.pos_embedding 400\n",
      "transformer.0.patch_to_embedding.0.weight 550\n",
      "transformer.0.patch_to_embedding.0.bias 550\n",
      "transformer.0.patch_to_embedding.1.weight 4400\n",
      "transformer.0.patch_to_embedding.1.bias 8\n",
      "transformer.0.patch_to_embedding.2.weight 8\n",
      "transformer.0.patch_to_embedding.2.bias 8\n",
      "transformer.0.attn_layers.layers.0.0.0.weight 8\n",
      "transformer.0.attn_layers.layers.0.0.0.bias 8\n",
      "transformer.0.attn_layers.layers.0.1.fn.ff.0.0.weight 64\n",
      "transformer.0.attn_layers.layers.0.1.fn.ff.0.0.bias 8\n",
      "transformer.0.attn_layers.layers.0.1.fn.ff.3.weight 64\n",
      "transformer.0.attn_layers.layers.0.1.fn.ff.3.bias 8\n",
      "transformer.0.attn_layers.layers.1.0.0.weight 8\n",
      "transformer.0.attn_layers.layers.1.0.0.bias 8\n",
      "transformer.0.attn_layers.layers.1.1.to_q.weight 8\n",
      "transformer.0.attn_layers.layers.1.1.to_k.weight 8\n",
      "transformer.0.attn_layers.layers.1.1.to_v.weight 8\n",
      "transformer.0.attn_layers.layers.1.1.to_out.weight 8\n",
      "transformer.0.attn_layers.layers.2.0.0.weight 8\n",
      "transformer.0.attn_layers.layers.2.0.0.bias 8\n",
      "transformer.0.attn_layers.layers.2.1.fn.ff.0.0.weight 64\n",
      "transformer.0.attn_layers.layers.2.1.fn.ff.0.0.bias 8\n",
      "transformer.0.attn_layers.layers.2.1.fn.ff.3.weight 64\n",
      "transformer.0.attn_layers.layers.2.1.fn.ff.3.bias 8\n",
      "transformer.0.attn_layers.rel_pos.relative_attention_bias.weight 32\n",
      "transformer.0.norm.weight 8\n",
      "transformer.0.norm.bias 8\n",
      "transformer.1.weight 32\n",
      "transformer.1.bias 4\n",
      "head.0.weight 32\n",
      "head.0.bias 4\n",
      "total:  10976\n"
     ]
    }
   ],
   "source": [
    "#params evaluation\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, np.prod(param.size()))\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print('total: ', params)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "outputs": [],
   "source": [
    "seed = 1330\n",
    "splits = 5\n",
    "lr=3e-4\n",
    "#fine tune\n",
    "# lr=1e-4\n",
    "\n",
    "skf = StratifiedKFold(n_splits=splits, random_state=seed, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ud = []\n",
    "\n",
    "#main-trianing-loop\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "\n",
    "    X_train, X_test = X[train_index], X[test_index],\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    TrainerTester.train_loop(model, optimizer, X_train, y_train, X_test, y_test, lr, ud, batch_size=16, iterations=2000)\n",
    "\n",
    "    out_values = TrainerTester.test_loop(model, X_test, y_test)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('model_states/test_model_states.txt'))\n",
    "# torch.save(model.state_dict(), 'model_states/test_model_states.txt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from modules.EdfHandler import EdfHandler\n",
    "#\n",
    "# epochs_list, labels_list = EdfHandler.getAllData(\n",
    "#     [\n",
    "#         \"C:\\\\Users\\\\davi2\\Desktop\\\\bci\\\\datasets_ufjf\\\\bci\\\\001.edf\",\n",
    "#         \"C:\\\\Users\\\\davi2\\Desktop\\\\bci\\\\datasets_ufjf\\\\bci\\\\002.edf\",\n",
    "#         \"C:\\\\Users\\\\davi2\\Desktop\\\\bci\\\\datasets_ufjf\\\\bci\\\\003.edf\",\n",
    "#         \"C:\\\\Users\\\\davi2\\Desktop\\\\bci\\\\datasets_ufjf\\\\bci\\\\004.edf\",\n",
    "#         \"C:\\\\Users\\\\davi2\\Desktop\\\\bci\\\\datasets_ufjf\\\\bci\\\\005.edf\",\n",
    "#         \"C:\\\\Users\\\\davi2\\Desktop\\\\bci\\\\datasets_ufjf\\\\bci\\\\006.edf\",\n",
    "#     ]\n",
    "# )\n",
    "#\n",
    "# for epochs, labels in zip(epochs_list, labels_list):\n",
    "#     labels = np.array(labels)\n",
    "#     labels[labels == 6] = 0\n",
    "#\n",
    "#     data = epochs.get_data()\n",
    "#     X = torch.tensor(data).to(dtype=torch.float32, device=device)\n",
    "#     y = torch.tensor(labels).to(dtype=torch.long, device=device)\n",
    "#\n",
    "#     print(X.shape[-1])\n",
    "#\n",
    "#     model = EEGNET(n_times=X.shape[-1], n_channels=len(epochs.picks), n_classes=len(set(labels)), patches_size=5)\n",
    "#     model = model.to(device=device)\n",
    "#\n",
    "#     seed = 1330\n",
    "#     splits = 5\n",
    "#     lr=3e-4\n",
    "#\n",
    "#     skf = StratifiedKFold(n_splits=splits, random_state=seed, shuffle=True)\n",
    "#\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "#\n",
    "#     ud = []\n",
    "#\n",
    "#     #main-trianing-loop\n",
    "#     for train_index, test_index in skf.split(X, y):\n",
    "#         X_train, X_test = X[train_index], X[test_index],\n",
    "#         y_train, y_test = y[train_index], y[test_index]\n",
    "#\n",
    "#         TrainerTester.train_loop(model, optimizer, X_train, y_train, X_test, y_test, lr, ud, batch_size=16, iterations=2000)\n",
    "#\n",
    "#         out_values = TrainerTester.test_loop(model, X_test, y_test)\n",
    "#         break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import re\n",
    "# # visualize histograms\n",
    "# plt.figure(figsize=(40, 20)) # width and height of the plot\n",
    "# legends = []\n",
    "# for name, params in model.named_parameters():\n",
    "#     t = params.grad\n",
    "#     # print(f'layer {name}: weight {tuple(params.shape)} | mean {t.mean()} | std {t.std()} | grad:data ratio { t.std() / params.std()}')\n",
    "#     hy, hx = torch.histogram(t, density=True)\n",
    "#     plt.plot(hx[:-1].detach(), hy.detach())\n",
    "#     legends.append(f'{name} {tuple(params.shape)}')\n",
    "# plt.legend(legends)\n",
    "# plt.title('weights gradient distribution')\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
