{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "\n",
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import math\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import importlib\n",
    "from einops import rearrange, reduce, repeat\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "class MyViTransformerWrapper(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            *,\n",
    "            image_size,\n",
    "            patch_size,\n",
    "            attn_layers,\n",
    "            channels,\n",
    "            num_classes = None,\n",
    "            dropout = 0.,\n",
    "            post_emb_norm = False,\n",
    "            emb_dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert isinstance(attn_layers, Encoder), 'attention layers must be an Encoder'\n",
    "        assert image_size % patch_size == 0, 'image dimensions must be divisible by the patch size'\n",
    "        dim = attn_layers.dim\n",
    "        print(\"dim: \", dim)\n",
    "        num_patches = (image_size // patch_size)\n",
    "        patch_dim = channels * patch_size\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(num_patches, dim))\n",
    "\n",
    "        self.patch_to_embedding = nn.Sequential(\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "        self.post_emb_norm = nn.LayerNorm(dim) if post_emb_norm else nn.Identity()\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.attn_layers = attn_layers\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.mlp_head = nn.Linear(dim, num_classes) if exists(num_classes) else nn.Identity()\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            img,\n",
    "            return_embeddings = False\n",
    "    ):\n",
    "        p = self.patch_size\n",
    "        img = img.unsqueeze(-2)\n",
    "        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = 1, p2 = p)\n",
    "        x = self.patch_to_embedding(x)\n",
    "        n = x.shape[1]\n",
    "        x = x + self.pos_embedding\n",
    "\n",
    "        x = self.post_emb_norm(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.attn_layers(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if not exists(self.mlp_head) or return_embeddings:\n",
    "            return x\n",
    "\n",
    "        x = x.mean(dim = -2)\n",
    "        return self.mlp_head(x)\n",
    "\n",
    "class LinearWithConstraint(nn.Linear):\n",
    "    def __init__(self, *config, max_norm=1, **kwconfig):\n",
    "        self.max_norm = max_norm\n",
    "        super(LinearWithConstraint, self).__init__(*config, **kwconfig)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data = torch.renorm(\n",
    "            self.weight.data, p=2, dim=0, maxnorm=self.max_norm\n",
    "        )\n",
    "        return super(LinearWithConstraint, self).forward(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.utils import initialize_weight\n",
    "from base.layers import Conv2dWithConstraint\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from x_transformers import TransformerWrapper, Encoder, ViTransformerWrapper\n",
    "\n",
    "\n",
    "class FeatureExtraction(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_channels,\n",
    "            kernel_length,\n",
    "            F1,\n",
    "            D,\n",
    "            F2,\n",
    "            pool1_stride,\n",
    "            pool2_stride,\n",
    "            dropout_rate,\n",
    "            weight_init_method=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Spectral\n",
    "        self.rearrange = Rearrange('b c w -> b 1 c w')\n",
    "        self.conv1 = nn.Conv2d(1, F1, (1, kernel_length), bias=False, padding='same')\n",
    "        self.batch1 = nn.BatchNorm2d(F1)\n",
    "\n",
    "        # Spatial\n",
    "        self.conv2 = Conv2dWithConstraint(F1, F1 * D, (n_channels, 1), bias=False, groups=F1)\n",
    "        self.batch2 = nn.BatchNorm2d(F1 * D)\n",
    "        self.activation1 = nn.ELU()\n",
    "        self.avgpool1 = nn.AvgPool2d((1, pool1_stride))\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Temporal\n",
    "        self.conv3 = nn.Conv2d(F1 * D, F2, (1, n_channels), bias=False, padding='same', groups=F2)\n",
    "        self.conv4 = nn.Conv2d(F2, F2, 1, bias=False)\n",
    "        self.batch3 = nn.BatchNorm2d(F2)\n",
    "        self.activation2 = nn.ELU()\n",
    "        self.avgpool2 = nn.AvgPool2d((1, pool2_stride))\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Classifier\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        initialize_weight(self, weight_init_method)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        out = self.rearrange(x)\n",
    "        # print(out.shape)\n",
    "        out = self.conv1(out)\n",
    "        # print(out.shape)\n",
    "        out = self.batch1(out)\n",
    "        # print(out.shape)\n",
    "\n",
    "        # Spatial\n",
    "        out = self.conv2(out)\n",
    "        # print(out.shape)\n",
    "        out = self.batch2(out)\n",
    "        # print(out.shape)\n",
    "        out = self.activation1(out)\n",
    "        # print(out.shape)\n",
    "        out = self.avgpool1(out)\n",
    "        # print(out.shape)\n",
    "        out = self.dropout1(out)\n",
    "        # print(out.shape)\n",
    "\n",
    "        # Temporal\n",
    "        out = self.conv3(out)\n",
    "        # print(out.shape)\n",
    "        out = self.conv4(out)\n",
    "        # print(out.shape)\n",
    "        out = self.batch3(out)\n",
    "        # print(out.shape)\n",
    "        out = self.activation2(out)\n",
    "        # print(out.shape)\n",
    "        out = self.avgpool2(out)\n",
    "        # print(out.shape)\n",
    "        out = self.dropout2(out)\n",
    "        # print(out.shape)\n",
    "        out = self.flatten(out)\n",
    "        # print(out.shape)\n",
    "\n",
    "        return out\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from einops.layers.torch import Reduce\n",
    "from x_transformers import ContinuousTransformerWrapper\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "class EEGNET(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_times,\n",
    "            n_classes,\n",
    "            n_channels,\n",
    "            kernel_length,\n",
    "            patches_size=5,\n",
    "            transformer_dim=4,\n",
    "            transformer_layers=1,\n",
    "            transformer_heads=1,\n",
    "            dropout_rate=0.5,\n",
    "            max_norm=0.25,\n",
    "            F1=8,\n",
    "            F2=16,\n",
    "            D=2,\n",
    "            pool1_stride=8,\n",
    "            pool2_stride=8,\n",
    "            n_hidden=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.feature_extraction_output = F2 * ((((n_times - pool1_stride) // pool1_stride + 1) - pool2_stride) // pool2_stride + 1)\n",
    "\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            FeatureExtraction(n_channels=n_channels, kernel_length=kernel_length, F1=F1, D=D, F2=F2, pool1_stride=pool1_stride, pool2_stride=pool2_stride, dropout_rate=dropout_rate),\n",
    "            nn.Linear(in_features=self.feature_extraction_output, out_features=n_hidden),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "\n",
    "        self.transformer = nn.Sequential(\n",
    "            MyViTransformerWrapper(\n",
    "                image_size=n_times,\n",
    "                patch_size=patches_size,\n",
    "                channels=n_channels,\n",
    "                dropout=dropout_rate,\n",
    "                attn_layers= Encoder(\n",
    "                    dim=transformer_dim,\n",
    "                    depth=transformer_layers,\n",
    "                    heads=transformer_heads,\n",
    "                    macaron=True,\n",
    "                    rel_pos_bias=True,\n",
    "                    attn_dim_head = 1,\n",
    "                    value_dim_head=1,\n",
    "                    dim_head=1,\n",
    "                    ff_mult=1,\n",
    "                ),\n",
    "            ),\n",
    "            nn.Linear(in_features=transformer_dim, out_features=n_hidden),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "\n",
    "        # self.spatial_transformer = nn.Sequential(\n",
    "        #     # Rearrange(\"b c t -> b 1 c t\"),\n",
    "        #     # nn.Conv2d(in_channels=1, out_channels=4, kernel_size=(8, 32), groups=1),\n",
    "        #     # nn.Conv2d(in_channels=4, out_channels=1, kernel_size=(8, 32), groups=1),\n",
    "        #     # nn.AdaptiveAvgPool2d((8, 64)),\n",
    "        #     # Rearrange(\"b 1 c t -> b t c\"),\n",
    "        #\n",
    "        #     nn.AdaptiveAvgPool1d(2), #layer do stop overfit\n",
    "        #     Rearrange(\"b c t -> b t c\"),\n",
    "        #     MyViTransformerWrapper(\n",
    "        #         image_size=n_channels,\n",
    "        #         patch_size=n_channels//2,\n",
    "        #         channels=2,\n",
    "        #         dropout=dropout_rate,\n",
    "        #         attn_layers=Encoder(\n",
    "        #             dim=transformer_dim,\n",
    "        #             depth=transformer_layers,\n",
    "        #             heads=transformer_heads,\n",
    "        #             macaron=True,\n",
    "        #             rel_pos_bias=True,\n",
    "        #             attn_dim_head = 32,\n",
    "        #         ),\n",
    "        #     ),\n",
    "        #     # nn.LayerNorm(transformer_dim),\n",
    "        #     # nn.Dropout(dropout_rate),\n",
    "        #     nn.Linear(in_features=transformer_dim, out_features=n_hidden),\n",
    "        #     nn.ELU(),\n",
    "        # )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            LinearWithConstraint(in_features=2*n_hidden, out_features=n_classes, max_norm=max_norm),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        out_values = {}\n",
    "        feature_extraction_result = self.feature_extraction(x)\n",
    "        transformer_result = self.transformer(x)\n",
    "        # spatial_transformer_result = self.spatial_transformer(x)\n",
    "        logits = self.head(torch.cat([\n",
    "            feature_extraction_result,\n",
    "            transformer_result,\n",
    "            # spatial_transformer_result\n",
    "        ], dim=-1))\n",
    "\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss, out_values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ------------------------------ bci competition dataset ------------------------------\n",
    "\n",
    "import mne\n",
    "from moabb.datasets import BNCI2014001\n",
    "from moabb.paradigms import MotorImagery\n",
    "# fmin = 4\n",
    "# fmax = 100\n",
    "fmin = 5\n",
    "fmax = 60\n",
    "tmin = 0\n",
    "tmax = None\n",
    "\n",
    "# Load the dataset\n",
    "# subjects = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "subjects = [1]\n",
    "dataset = BNCI2014001()\n",
    "events = ['left_hand', 'right_hand', 'feet', 'tongue']\n",
    "labels_to_int = {'left_hand':0, 'right_hand':1, 'feet':2, 'tongue':3}\n",
    "paradigm = MotorImagery(\n",
    "    events=events, n_classes=len(events), fmin=fmin, fmax=fmax, tmin=tmin, tmax=tmax\n",
    ")\n",
    "\n",
    "data, labels, _ = paradigm.get_data(dataset=dataset, subjects=subjects)\n",
    "data = data[:, :, 500:-1]\n",
    "\n",
    "labels = [labels_to_int[label] for label in labels]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = torch.tensor(data).to(dtype=torch.float32, device=device)\n",
    "y = torch.tensor(labels).to(dtype=torch.long, device=device)\n",
    "\n",
    "#sliding window\n",
    "# slide = np.linspace(0, 3.5, 36)\n",
    "#\n",
    "# x_slides = []\n",
    "# y_slides = []\n",
    "# for i in slide:\n",
    "#     x_slides.append(X[:, :, int(i*250):int((i+1)*250)])\n",
    "#     y_slides.append(y)\n",
    "#\n",
    "#\n",
    "# X = torch.concat(x_slides)\n",
    "# y = torch.concat(y_slides)\n",
    "\n",
    "#downsmaple\n",
    "# X = X[:,:,::2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#dataset info\n",
    "channels = 22\n",
    "classes = 4\n",
    "samples=500\n",
    "frequency=250"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = EEGNET(n_times=samples, n_channels=channels, n_classes=classes, kernel_length=frequency//2)\n",
    "model = model.to(device=device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#params evaluation\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, np.prod(param.size()))\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print('total: ', params)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed = 1330\n",
    "splits = 5\n",
    "\n",
    "skf = StratifiedKFold(n_splits=splits, random_state=seed, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from modules.TrainTester import TrainerTester\n",
    "\n",
    "ud = []\n",
    "\n",
    "#main-trianing-loop\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    TrainerTester.train_loop(model, optimizer, X_train, y_train, X_test, y_test, 1, ud, batch_size=32, iterations=2000)\n",
    "\n",
    "    out_values = TrainerTester.test_loop(model, X_test, y_test)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('model_states/test_model_states.txt'))\n",
    "# torch.save(model.state_dict(), 'model_states/test_model_states.txt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from modules.EdfHandler import EdfHandler\n",
    "#\n",
    "# epochs_list, labels_list = EdfHandler.getAllData(\n",
    "#     [\n",
    "#         \"C:\\\\Users\\\\davi2\\Desktop\\\\bci\\\\datasets_ufjf\\\\bci\\\\001.edf\",\n",
    "#         \"C:\\\\Users\\\\davi2\\Desktop\\\\bci\\\\datasets_ufjf\\\\bci\\\\002.edf\",\n",
    "#         \"C:\\\\Users\\\\davi2\\Desktop\\\\bci\\\\datasets_ufjf\\\\bci\\\\003.edf\",\n",
    "#         \"C:\\\\Users\\\\davi2\\Desktop\\\\bci\\\\datasets_ufjf\\\\bci\\\\004.edf\",\n",
    "#         \"C:\\\\Users\\\\davi2\\Desktop\\\\bci\\\\datasets_ufjf\\\\bci\\\\005.edf\",\n",
    "#         \"C:\\\\Users\\\\davi2\\Desktop\\\\bci\\\\datasets_ufjf\\\\bci\\\\006.edf\",\n",
    "#     ]\n",
    "# )\n",
    "#\n",
    "# for epochs, labels in zip(epochs_list, labels_list):\n",
    "#     labels = np.array(labels)\n",
    "#     labels[labels == 6] = 0\n",
    "#\n",
    "#     data = epochs.get_data()\n",
    "#     X = torch.tensor(data).to(dtype=torch.float32, device=device)\n",
    "#     X = X[:,:,:-1]\n",
    "#     y = torch.tensor(labels).to(dtype=torch.long, device=device)\n",
    "#\n",
    "#     print(X.shape[-1])\n",
    "#\n",
    "#     model = EEGNET(n_times=X.shape[-1], n_channels=len(epochs.picks), n_classes=len(set(labels)), kernel_length=epochs.info[\"sfreq\"]//2)\n",
    "#     model = model.to(device=device)\n",
    "#\n",
    "#     seed = 1330\n",
    "#     splits = 5\n",
    "#     lr=3e-4\n",
    "#\n",
    "#     skf = StratifiedKFold(n_splits=splits, random_state=seed, shuffle=True)\n",
    "#\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "#\n",
    "#     ud = []\n",
    "#\n",
    "#     #main-trianing-loop\n",
    "#     for train_index, test_index in skf.split(X, y):\n",
    "#         X_train, X_test = X[train_index], X[test_index],\n",
    "#         y_train, y_test = y[train_index], y[test_index]\n",
    "#\n",
    "#         TrainerTester.train_loop(model, optimizer, X_train, y_train, X_test, y_test, lr, ud, batch_size=16, iterations=2000)\n",
    "#\n",
    "#         out_values = TrainerTester.test_loop(model, X_test, y_test)\n",
    "#         break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import re\n",
    "# # visualize histograms\n",
    "# plt.figure(figsize=(40, 20)) # width and height of the plot\n",
    "# legends = []\n",
    "# for name, params in model.named_parameters():\n",
    "#     t = params.grad\n",
    "#     # print(f'layer {name}: weight {tuple(params.shape)} | mean {t.mean()} | std {t.std()} | grad:data ratio { t.std() / params.std()}')\n",
    "#     hy, hx = torch.histogram(t, density=True)\n",
    "#     plt.plot(hx[:-1].detach(), hy.detach())\n",
    "#     legends.append(f'{name} {tuple(params.shape)}')\n",
    "# plt.legend(legends)\n",
    "# plt.title('weights gradient distribution')\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
