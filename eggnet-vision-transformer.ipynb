{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "\n",
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import math\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import importlib\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def unfold_output_size(n_times, patches, step):\n",
    "    return (n_times - patches) / step + 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from modules.BciDataHandler import BciDataHandler\n",
    "\n",
    "data_handler = BciDataHandler()\n",
    "data_handler.instantiate_dataset()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ------------------------------ bci competition dataset ------------------------------\n",
    "all_subject_epochs = mne.concatenate_epochs(list(data_handler.subjects_epochs.values()))\n",
    "all_labels = all_subject_epochs.events[:, -1] - 1\n",
    "\n",
    "epochs = data_handler.subjects_epochs[1]\n",
    "labels = np.array(data_handler.subjects_labels[1]) - 1\n",
    "\n",
    "# epochs = all_subject_epochs\n",
    "# labels = all_labels\n",
    "# labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# -------------------------------- ufjf dataset --------------------------------------\n",
    "# from modules.EdfHandler import EdfHandler\n",
    "#\n",
    "# epochs, labels = EdfHandler.getAllData([\"C:\\\\Users\\\\davi2\\Desktop\\\\bci\\\\datasets_ufjf\\\\bci\\\\001.edf\"])\n",
    "# epochs = epochs[0]\n",
    "# labels = np.array(labels[0])\n",
    "# labels[labels == 6] = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#----------------------- physionet dataset -------------------------------------\n",
    "# import mne\n",
    "# from mne import Epochs, pick_types, events_from_annotations\n",
    "# from mne.channels import make_standard_montage\n",
    "# from mne.io import concatenate_raws, read_raw_edf\n",
    "# from mne.datasets import eegbci\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# # Set parameters and read data\n",
    "\n",
    "# avoid classification of evoked responses by using epochs that start 1s after\n",
    "# cue onset.\n",
    "# tmin, tmax = -1., 4.\n",
    "# event_id = dict(handsOrLeft=2, feetOrRight=3)\n",
    "#\n",
    "# def get_physionet_data(subject, runs):\n",
    "#\n",
    "#     raw_fnames = eegbci.load_data(subject, runs)\n",
    "#     raw = concatenate_raws([read_raw_edf(f, preload=True) for f in raw_fnames])\n",
    "#     eegbci.standardize(raw)  # set channel names\n",
    "#     montage = make_standard_montage('standard_1005')\n",
    "#     raw.set_montage(montage)\n",
    "#\n",
    "#     # Apply band-pass filter\n",
    "#     raw.filter(7., 30., fir_design='firwin', skip_by_annotation='edge')\n",
    "#\n",
    "#     events, _ = events_from_annotations(raw)\n",
    "#\n",
    "#     picks = pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False,\n",
    "#                        exclude='bads')\n",
    "#\n",
    "#     # Read epochs (train will be done only between 1 and 2s)\n",
    "#     # Testing will be done with a running classifier\n",
    "#     epochs = Epochs(raw, events, event_id, tmin, tmax, proj=True, picks=picks,\n",
    "#                     baseline=None, preload=True)\n",
    "#\n",
    "#     epochs_data = epochs.copy().crop(tmin=1., tmax=2.)\n",
    "#\n",
    "#     labels = epochs.events[:, -1] - 2\n",
    "#\n",
    "#     return epochs_data, labels\n",
    "\n",
    "\n",
    "# [6, 10, 14] hands vs feet\n",
    "#[4, 8, 12] left vs right hand\n",
    "# X_hf, y_hf = get_physionet_data(subject=1, runs=[6, 10, 14])\n",
    "# X_lr, y_lr = get_physionet_data(subject=1, runs=[4, 8, 12])\n",
    "#\n",
    "# epochs = mne.concatenate_epochs([X_hf, X_lr])\n",
    "# labels = np.concatenate([y_hf, y_lr+2])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def plot_psd(data, axis, label, color):\n",
    "#     psds, freqs = mne.time_frequency.psd_array_multitaper(data, sfreq=sfreq,\n",
    "#                                                           fmin=0.1, fmax=100)\n",
    "#     psds = 10. * np.log10(psds)\n",
    "#     psds_mean = psds.mean(0).mean(0)\n",
    "#     axis.plot(freqs, psds_mean, color=color, label=label)\n",
    "#\n",
    "#\n",
    "# _, ax = plt.subplots()\n",
    "# plot_psd(X, ax, 'original', 'k')\n",
    "# plot_psd(X_tr.numpy(), ax, 'shifted', 'r')\n",
    "#\n",
    "# ax.set(title='Multitaper PSD (gradiometers)', xlabel='Frequency (Hz)',\n",
    "#        ylabel='Power Spectral Density (dB)')\n",
    "# ax.legend()\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DepthWiseConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, kernel_size, kernels_per_layer, bias=False):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels=in_channels, out_channels=in_channels * kernels_per_layer,\n",
    "                                   kernel_size=kernel_size, groups=in_channels, bias=bias, padding='same')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.depthwise(x)\n",
    "\n",
    "\n",
    "class PointWiseConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernels_per_layer=1, bias=False):\n",
    "        super().__init__()\n",
    "        self.pointwise = nn.Conv2d(in_channels=in_channels * kernels_per_layer, out_channels=out_channels,\n",
    "                                   kernel_size=(1, 1), bias=bias, padding=\"valid\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pointwise(x)\n",
    "\n",
    "\n",
    "class MaxNormLayer(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, max_norm=1.0):\n",
    "        super(MaxNormLayer, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.max_norm = max_norm\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.max_norm is not None:\n",
    "            with torch.no_grad():\n",
    "                self.weight.data = torch.renorm(\n",
    "                    self.weight.data, p=2, dim=0, maxnorm=self.max_norm\n",
    "                )\n",
    "        return super(MaxNormLayer, self).forward(x)\n",
    "\n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, kernels_per_layer=1, bias=False):\n",
    "        super().__init__()\n",
    "        self.depthwise = DepthWiseConv2d(in_channels=in_channels, kernels_per_layer=kernels_per_layer,\n",
    "                                         kernel_size=kernel_size, bias=bias)\n",
    "        self.pointwise = PointWiseConv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                         kernels_per_layer=kernels_per_layer, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class ViewConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view((x.shape[0], x.shape[1], 1, x.shape[2]))\n",
    "\n",
    "\n",
    "class ToTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view((x.shape[1], x.shape[0], x.shape[2]))\n",
    "\n",
    "\n",
    "class Unsqueeze(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view((x.shape[0], x.shape[1], 1))\n",
    "\n",
    "\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_times,\n",
    "            patches_size,\n",
    "            embedding_dim,\n",
    "            dropout_rate,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.patches_size = patches_size\n",
    "        self.unfold_output = int(unfold_output_size(n_times, self.patches_size, self.patches_size))\n",
    "        self.embedding = nn.Linear(in_features=patches_size, out_features=embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.positional_encode_table = nn.Embedding(num_embeddings=self.unfold_output, embedding_dim=embedding_dim)\n",
    "\n",
    "    # output shape -> [Batches, embeding dim, pathces,]\n",
    "    def forward(self, x):\n",
    "        out = x.unfold(-1, self.patches_size, int(self.patches_size))\n",
    "        out = out.unsqueeze(-2)\n",
    "        out = self.embedding(out)\n",
    "\n",
    "        out = out.squeeze(-2)\n",
    "        positional_result = out + self.positional_encode_table((torch.arange(self.unfold_output)))\n",
    "        out = self.dropout(positional_result)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            patches,\n",
    "            transformer_ffd,\n",
    "            nhead=4,\n",
    "            num_layers=2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.Sequential(\n",
    "            nn.TransformerEncoder(\n",
    "                nn.TransformerEncoderLayer(d_model=patches, dim_feedforward=transformer_ffd, nhead=nhead,\n",
    "                                           batch_first=True),\n",
    "                num_layers=num_layers,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.transformer(x)\n",
    "\n",
    "\n",
    "class ConvolutionSimplifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(output_size=1),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Flatten(),\n",
    "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class FeatureExtraction(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_channels,\n",
    "            kernel_length,\n",
    "            F1,\n",
    "            D,\n",
    "            F2,\n",
    "            pool1_stride,\n",
    "            pool2_stride,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            ViewConv(),\n",
    "            nn.Conv2d(in_channels=n_channels, out_channels=F1, kernel_size=(1, kernel_length), bias=False,\n",
    "                      padding='same'),\n",
    "            nn.BatchNorm2d(num_features=F1, momentum=0.01, eps=0.001, track_running_stats=False),\n",
    "            DepthWiseConv2d(in_channels=F1, kernel_size=(n_channels, 1), kernels_per_layer=D, bias=False),\n",
    "            nn.BatchNorm2d(num_features=F1 * D, momentum=0.01, eps=0.001, track_running_stats=False),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d(kernel_size=(1, pool1_stride), stride=pool1_stride),\n",
    "            SeparableConv2d(in_channels=F1 * D, kernel_size=(1, 16), out_channels=F2, bias=False),\n",
    "            nn.BatchNorm2d(num_features=F2, momentum=0.01, eps=0.001, track_running_stats=False),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d(kernel_size=(1, pool2_stride), stride=pool2_stride),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class EEGNET(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_times,\n",
    "            n_classes,\n",
    "            n_channels,\n",
    "            patches_size=8,\n",
    "            embedding_dim=16,\n",
    "            dropout_rate=0.1,\n",
    "            transformer_ffd=128,\n",
    "            max_norm=0.25,\n",
    "            F1=8,\n",
    "            F2=16,\n",
    "            D=2,\n",
    "            pool1_stride=4,\n",
    "            pool2_stride=8,\n",
    "            kernel_length=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.feature_extraction_output = F2 * ((((n_times - pool1_stride) // pool1_stride + 1) - pool2_stride) // pool2_stride + 1)\n",
    "\n",
    "        self.patches_num = int(unfold_output_size(self.feature_extraction_output, patches_size, patches_size))\n",
    "\n",
    "        self.feature_extraction = FeatureExtraction(n_channels=n_channels, kernel_length=kernel_length, F1=F1, D=D, F2=F2, pool1_stride=pool1_stride, pool2_stride=pool2_stride)\n",
    "\n",
    "        self.embedding = EmbeddingLayer(n_times=self.feature_extraction_output, patches_size=patches_size, embedding_dim=embedding_dim, dropout_rate=dropout_rate)\n",
    "\n",
    "        self.transformer = VisionTransformer(patches=embedding_dim, transformer_ffd=transformer_ffd)\n",
    "\n",
    "        # self.conv_simplifier = ConvolutionSimplifier(embedding_dim=embedding_dim, dropout_rate=dropout_rate)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim, out_features=1),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.ELU(),\n",
    "            nn.Flatten(),\n",
    "            MaxNormLayer(in_features=self.patches_num, out_features=n_classes, max_norm=max_norm),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        out_values = {}\n",
    "        out = self.feature_extraction(x)\n",
    "        out = self.embedding(out)\n",
    "        out = self.transformer(out)\n",
    "        # out = self.conv_simplifier(out)\n",
    "        logits = self.head(out)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss, out_values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#data augmentation\n",
    "from braindecode.augmentation import FrequencyShift\n",
    "from braindecode.augmentation import GaussianNoise\n",
    "\n",
    "sfreq = epochs.info['sfreq']\n",
    "\n",
    "freq_shift = FrequencyShift(\n",
    "    probability=0.5,  # defines the probability of actually modifying the input\n",
    "    sfreq=sfreq,\n",
    "    max_delta_freq=2.  # the frequency shifts are sampled now between -2 and 2 Hz\n",
    ")\n",
    "\n",
    "gauss_noise = GaussianNoise(\n",
    "    probability=0.5,\n",
    "    std=0.01\n",
    ")\n",
    "\n",
    "transforms = {\n",
    "    'freq': freq_shift,\n",
    "    'gauss': gauss_noise\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = torch.tensor(epochs.get_data()).to(dtype=torch.float32, device=device)\n",
    "y = torch.tensor(labels).to(dtype=torch.long, device=device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = EEGNET(n_times=X.shape[-1], n_channels=len(epochs.picks), n_classes=len(set(labels)))\n",
    "model = model.to(device=device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed = 1330\n",
    "splits = 5\n",
    "lr=3e-3\n",
    "\n",
    "skf = StratifiedKFold(n_splits=splits, random_state=seed, shuffle=True)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from modules.TrainTester import TrainerTester\n",
    "\n",
    "ud = []\n",
    "\n",
    "#main-trianing-loop\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index],\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    TrainerTester.train_loop(model, optimizer, X_train, y_train, X_test, y_test, lr, ud, batch_size=32, iterations=2000)\n",
    "\n",
    "    out_values = TrainerTester.test_loop(model, X_test, y_test)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#params evaluation\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, np.prod(param.size()))\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print('total: ', params)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meta_step_size = 0.25\n",
    "\n",
    "meta_iters = 1000\n",
    "\n",
    "eval_interval = 1\n",
    "train_shots = 40\n",
    "eval_shots = 4\n",
    "classes = len(set(labels))\n",
    "\n",
    "batch_size = 1\n",
    "#obs: total shots = classes * shots\n",
    "\n",
    "n_times=X.shape[-1]\n",
    "n_channels=len(epochs.picks)\n",
    "\n",
    "seed = 1330\n",
    "splits = 5\n",
    "lr=1e-4\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)\n",
    "\n",
    "train_index, test_index = skf.split(X, y).__next__()\n",
    "X_train, X_test = X[train_index], X[test_index],\n",
    "y_train, y_test = y[train_index], y[test_index]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, training):\n",
    "        split = \"train\" if training else \"test\"\n",
    "\n",
    "        if split:\n",
    "            X_dataset, y_dataset = X_test, y_test\n",
    "        else:\n",
    "            X_dataset, y_dataset = X_train, y_train\n",
    "\n",
    "        self.data = {}\n",
    "\n",
    "        for value, label in zip(X_dataset, y_dataset):\n",
    "            if label not in self.data:\n",
    "                self.data[label] = []\n",
    "            self.data[label].append(value)\n",
    "        self.labels = list(self.data.keys())\n",
    "\n",
    "    def get_mini_dataset(self, shots, num_classes, split=False):\n",
    "        temp_labels = torch.zeros((num_classes * shots))\n",
    "        temp_X = torch.zeros((num_classes * shots, n_channels, n_times))\n",
    "        if split:\n",
    "            test_labels = torch.zeros((num_classes * eval_shots))\n",
    "            test_X = torch.zeros((num_classes * eval_shots, n_channels, n_times))\n",
    "\n",
    "        # Get a random subset of labels from the entire label set.\n",
    "        label_subset = random.choices(self.labels, k=num_classes)\n",
    "        for class_idx, class_obj in enumerate(label_subset):\n",
    "            # Use enumerated index value as a temporary label for mini-batch in\n",
    "            # few shot learning.\n",
    "            temp_labels[class_idx * shots : (class_idx + 1) * shots] = class_idx\n",
    "            # If creating a split dataset for testing, select an extra sample from each\n",
    "            # label to create the test dataset.\n",
    "            if split:\n",
    "                test_labels[class_idx] = class_idx\n",
    "                X_to_split = torch.stack(random.choices(self.data[label_subset[class_idx]], k=shots + 1))\n",
    "                test_X[class_idx] = X_to_split[-1]\n",
    "                temp_X[class_idx * shots : (class_idx + 1) * shots] = X_to_split[:-1]\n",
    "            else:\n",
    "                # For each index in the randomly selected label_subset, sample the\n",
    "                # necessary number of images.\n",
    "                temp_X[class_idx * shots : (class_idx + 1) * shots] = \\\n",
    "                    torch.stack(random.choices(self.data[label_subset[class_idx]], k=shots))\n",
    "\n",
    "        temp_X, temp_labels = unison_shuffled_copies(temp_X, temp_labels)\n",
    "        temp_X, temp_labels = torch.stack(temp_X.chunk(batch_size)), torch.stack(temp_labels.chunk(batch_size))\n",
    "        dataset = zip(temp_X, temp_labels)\n",
    "\n",
    "        if split:\n",
    "            test_X, test_labels = unison_shuffled_copies( test_X, test_labels)\n",
    "            return dataset, test_X, test_labels\n",
    "        return dataset\n",
    "\n",
    "train_dataset = Dataset(training=True)\n",
    "test_dataset = Dataset(training=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training = []\n",
    "testing = []\n",
    "for meta_iter in range(meta_iters):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    frac_done = meta_iter / meta_iters\n",
    "\n",
    "    cur_meta_step_size = (1 - frac_done) * meta_step_size\n",
    "\n",
    "    old_vars = model.state_dict()\n",
    "\n",
    "    mini_dataset = train_dataset.get_mini_dataset(\n",
    "        train_shots, classes\n",
    "    )\n",
    "\n",
    "    for X_values, y_labels in mini_dataset:\n",
    "        y_labels = y_labels.to(dtype=torch.long)\n",
    "        preds, loss, out_values = model(X_values, y_labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    new_vars = model.state_dict()\n",
    "\n",
    "    for key, var in new_vars.items():\n",
    "        new_vars[key] = old_vars[key] + ((new_vars[key] - old_vars[key]) * cur_meta_step_size)\n",
    "\n",
    "    model.load_state_dict(new_vars)\n",
    "\n",
    "    # Evaluation loop\n",
    "    if meta_iter % eval_interval == 0:\n",
    "        accuracies = []\n",
    "        for dataset in (train_dataset, test_dataset):\n",
    "            # print(\"test dataset reset!\\n\")\n",
    "            # Sample a mini dataset from the full dataset.\n",
    "            train_set, test_X, test_labels = dataset.get_mini_dataset(\n",
    "                eval_shots, classes, split=True\n",
    "            )\n",
    "            old_vars = model.state_dict()\n",
    "\n",
    "            for X_values, y_labels in train_set:\n",
    "                y_labels = y_labels.to(dtype=torch.long)\n",
    "\n",
    "                preds, test_loss, out_values = model(X_values, y_labels)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                test_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            test_labels = test_labels.to(dtype=torch.long)\n",
    "            test_preds, test_loss, test_out_values = model(test_X, test_labels)\n",
    "\n",
    "            accuracy = (test_preds.argmax(1) == test_labels).type(torch.float32).sum().item() / test_labels.shape[0]\n",
    "            accuracies.append(accuracy)\n",
    "\n",
    "            model.load_state_dict(old_vars)\n",
    "\n",
    "        training.append(accuracies[0])\n",
    "        testing.append(accuracies[1])\n",
    "\n",
    "        if meta_iter % 100 == 0:\n",
    "            print(f\"batch {meta_iter}: train={accuracies[0]} test={accuracies[1]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# First, some preprocessing to smooth the training and testing arrays for display.\n",
    "window_length = 100\n",
    "train_s = np.r_[\n",
    "    training[window_length - 1 : 0 : -1], training, training[-1:-window_length:-1]\n",
    "]\n",
    "test_s = np.r_[\n",
    "    testing[window_length - 1 : 0 : -1], testing, testing[-1:-window_length:-1]\n",
    "]\n",
    "w = np.hamming(window_length)\n",
    "train_y = np.convolve(w / w.sum(), train_s, mode=\"valid\")\n",
    "test_y = np.convolve(w / w.sum(), test_s, mode=\"valid\")\n",
    "\n",
    "# Display the training accuracies.\n",
    "x = np.arange(0, len(test_y), 1)\n",
    "plt.plot(x, test_y, x, train_y)\n",
    "plt.legend([\"test\", \"train\"])\n",
    "plt.grid()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('model_states/test_model_states.txt'))\n",
    "# torch.save(model.state_dict(), 'model_states/test_model_states.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
