{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "\n",
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import math\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import importlib\n",
    "from einops import rearrange, reduce, repeat\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def unfold_output_size(n_times, patches, step):\n",
    "    return (n_times - patches) / step + 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from modules.BciDataHandler import BciDataHandler\n",
    "\n",
    "data_handler = BciDataHandler()\n",
    "data_handler.instantiate_dataset()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ------------------------------ bci competition dataset ------------------------------\n",
    "all_subject_epochs = mne.concatenate_epochs(list(data_handler.subjects_epochs.values()))\n",
    "all_labels = all_subject_epochs.events[:, -1] - 1\n",
    "\n",
    "epochs = data_handler.subjects_epochs[1]\n",
    "labels = np.array(data_handler.subjects_labels[1]) - 1\n",
    "\n",
    "# epochs = all_subject_epochs\n",
    "# labels = all_labels\n",
    "# labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# -------------------------------- ufjf dataset --------------------------------------\n",
    "# from modules.EdfHandler import EdfHandler\n",
    "#\n",
    "# epochs, labels = EdfHandler.getAllData([\"C:\\\\Users\\\\davi2\\Desktop\\\\bci\\\\datasets_ufjf\\\\bci\\\\001.edf\"])\n",
    "# epochs = epochs[0]\n",
    "# labels = np.array(labels[0])\n",
    "# labels[labels == 6] = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#----------------------- physionet dataset -------------------------------------\n",
    "# import mne\n",
    "# from mne import Epochs, pick_types, events_from_annotations\n",
    "# from mne.channels import make_standard_montage\n",
    "# from mne.io import concatenate_raws, read_raw_edf\n",
    "# from mne.datasets import eegbci\n",
    "#\n",
    "#\n",
    "# #############################################################################\n",
    "# # Set parameters and read data\n",
    "#\n",
    "# # avoid classification of evoked responses by using epochs that start 1s after\n",
    "# # cue onset.\n",
    "# tmin, tmax = -1., 4.\n",
    "# event_id = dict(handsOrLeft=2, feetOrRight=3)\n",
    "#\n",
    "# def get_physionet_data(subject, runs):\n",
    "#\n",
    "#     raw_fnames = eegbci.load_data(subject, runs)\n",
    "#     raw = concatenate_raws([read_raw_edf(f, preload=True) for f in raw_fnames])\n",
    "#     eegbci.standardize(raw)  # set channel names\n",
    "#     montage = make_standard_montage('standard_1005')\n",
    "#     raw.set_montage(montage)\n",
    "#\n",
    "#     # Apply band-pass filter\n",
    "#     raw.filter(7., 30., fir_design='firwin', skip_by_annotation='edge')\n",
    "#\n",
    "#     events, _ = events_from_annotations(raw)\n",
    "#\n",
    "#     picks = pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False,\n",
    "#                        exclude='bads')\n",
    "#\n",
    "#     # Read epochs (train will be done only between 1 and 2s)\n",
    "#     # Testing will be done with a running classifier\n",
    "#     epochs = Epochs(raw, events, event_id, tmin, tmax, proj=True, picks=picks,\n",
    "#                     baseline=None, preload=True)\n",
    "#\n",
    "#     epochs_data = epochs.copy().crop(tmin=1., tmax=2.)\n",
    "#\n",
    "#     labels = epochs.events[:, -1] - 2\n",
    "#\n",
    "#     return epochs_data, labels\n",
    "#\n",
    "#\n",
    "# # [6, 10, 14] hands vs feet\n",
    "# # [4, 8, 12] left vs right hand\n",
    "# X_hf, y_hf = get_physionet_data(subject=1, runs=[6, 10, 14])\n",
    "# X_lr, y_lr = get_physionet_data(subject=1, runs=[4, 8, 12])\n",
    "#\n",
    "# epochs = mne.concatenate_epochs([X_hf, X_lr])\n",
    "# labels = np.concatenate([y_hf, y_lr+2])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def plot_psd(data, axis, label, color):\n",
    "#     psds, freqs = mne.time_frequency.psd_array_multitaper(data, sfreq=sfreq,\n",
    "#                                                           fmin=0.1, fmax=100)\n",
    "#     psds = 10. * np.log10(psds)\n",
    "#     psds_mean = psds.mean(0).mean(0)\n",
    "#     axis.plot(freqs, psds_mean, color=color, label=label)\n",
    "#\n",
    "#\n",
    "# _, ax = plt.subplots()\n",
    "# plot_psd(X, ax, 'original', 'k')\n",
    "# plot_psd(X_tr.numpy(), ax, 'shifted', 'r')\n",
    "#\n",
    "# ax.set(title='Multitaper PSD (gradiometers)', xlabel='Frequency (Hz)',\n",
    "#        ylabel='Power Spectral Density (dB)')\n",
    "# ax.legend()\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "class MyViTransformerWrapper(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            *,\n",
    "            image_size,\n",
    "            patch_size,\n",
    "            attn_layers,\n",
    "            channels,\n",
    "            num_classes = None,\n",
    "            dropout = 0.,\n",
    "            post_emb_norm = False,\n",
    "            emb_dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert isinstance(attn_layers, Encoder), 'attention layers must be an Encoder'\n",
    "        assert image_size % patch_size == 0, 'image dimensions must be divisible by the patch size'\n",
    "        dim = attn_layers.dim\n",
    "        num_patches = (image_size // patch_size)\n",
    "        patch_dim = channels * patch_size\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(num_patches, dim))\n",
    "\n",
    "        self.patch_to_embedding = nn.Sequential(\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "        self.post_emb_norm = nn.LayerNorm(dim) if post_emb_norm else nn.Identity()\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.attn_layers = attn_layers\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.mlp_head = nn.Linear(dim, num_classes) if exists(num_classes) else nn.Identity()\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            img,\n",
    "            return_embeddings = False\n",
    "    ):\n",
    "        p = self.patch_size\n",
    "        img = img.unsqueeze(-2)\n",
    "        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = 1, p2 = p)\n",
    "        x = self.patch_to_embedding(x)\n",
    "        n = x.shape[1]\n",
    "        x = x + self.pos_embedding\n",
    "\n",
    "        x = self.post_emb_norm(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.attn_layers(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if not exists(self.mlp_head) or return_embeddings:\n",
    "            return x\n",
    "\n",
    "        x = x.mean(dim = -2)\n",
    "        return self.mlp_head(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.utils import initialize_weight\n",
    "from base.layers import Conv2dWithConstraint\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from x_transformers import TransformerWrapper, Encoder, ViTransformerWrapper\n",
    "\n",
    "\n",
    "class DepthWiseConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, bias=False):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                   kernel_size=kernel_size, groups=in_channels, bias=bias, padding='same')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.depthwise(x)\n",
    "\n",
    "\n",
    "class PointWiseConv2d(nn.Module):\n",
    "    def __init__(self, out_channels,  bias=False):\n",
    "        super().__init__()\n",
    "        self.pointwise = nn.Conv2d(in_channels=out_channels, out_channels=out_channels,\n",
    "                                   kernel_size=1, bias=bias, padding=\"valid\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pointwise(x)\n",
    "\n",
    "\n",
    "class MaxNormLayer(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, max_norm=1.0):\n",
    "        super(MaxNormLayer, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.max_norm = max_norm\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.max_norm is not None:\n",
    "            with torch.no_grad():\n",
    "                self.weight.data = torch.renorm(\n",
    "                    self.weight.data, p=2, dim=0, maxnorm=self.max_norm\n",
    "                )\n",
    "        return super(MaxNormLayer, self).forward(x)\n",
    "\n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, kernels_per_layer=1, bias=False):\n",
    "        super().__init__()\n",
    "        self.depthwise = DepthWiseConv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                         kernel_size=kernel_size, bias=bias)\n",
    "        self.pointwise = PointWiseConv2d(out_channels=out_channels, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViewConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view((x.shape[0], 1, x.shape[1], x.shape[2]))\n",
    "\n",
    "\n",
    "\n",
    "class FeatureExtraction(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_channels,\n",
    "            kernel_length,\n",
    "            F1,\n",
    "            D,\n",
    "            F2,\n",
    "            pool1_stride,\n",
    "            pool2_stride,\n",
    "            dropout_rate,\n",
    "            weight_init_method=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        pooling_layer = dict(max=nn.MaxPool2d, mean=nn.AvgPool2d)['mean']\n",
    "\n",
    "        if F2 == 'auto':\n",
    "            F2 = F1 * D\n",
    "\n",
    "        # Spectral\n",
    "        self.spectral = nn.Sequential(\n",
    "            Rearrange('b c w -> b 1 c w'),\n",
    "            nn.Conv2d(1, F1, (1, kernel_length), bias=False, padding='same'),\n",
    "            nn.BatchNorm2d(F1)\n",
    "        )\n",
    "\n",
    "        # Spatial\n",
    "        self.spatial = nn.Sequential(\n",
    "            Conv2dWithConstraint(F1, F1 * D, (n_channels, 1), bias=False, groups=F1),\n",
    "            nn.BatchNorm2d(F1 * D),\n",
    "            nn.ELU(),\n",
    "            pooling_layer((1, pool1_stride)),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        # Temporal\n",
    "        self.temporal = nn.Sequential(\n",
    "            nn.Conv2d(F1 * D, F2, (1, n_channels), bias=False, padding='same', groups=F1 * D),\n",
    "            nn.Conv2d(F2, F2, 1, bias=False),\n",
    "            nn.BatchNorm2d(F2),\n",
    "            nn.ELU(),\n",
    "            pooling_layer((1, pool2_stride)),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        initialize_weight(self, weight_init_method)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.spectral(x)\n",
    "        out = self.spatial(out)\n",
    "        out = self.temporal(out)\n",
    "        out = self.classifier(out)\n",
    "        return out\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from base.layers import LinearWithConstraint\n",
    "from einops.layers.torch import Reduce\n",
    "from x_transformers import ContinuousTransformerWrapper\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, \"b w e -> w b e\")\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        x = rearrange(x, \"w b e -> b w e\")\n",
    "        return self.dropout(x)\n",
    "\n",
    "class EEGNET(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_times,\n",
    "            n_classes,\n",
    "            n_channels,\n",
    "            patches_size=9,\n",
    "            transformer_dim=32,\n",
    "            transformer_layers=1,\n",
    "            transformer_heads=1,\n",
    "            dropout_rate=0.5,\n",
    "            final_dropout_rate=0.1,\n",
    "            max_norm=0.25,\n",
    "            F1=8,\n",
    "            F2=16,\n",
    "            D=2,\n",
    "            pool1_stride=4,\n",
    "            pool2_stride=8,\n",
    "            kernel_length=4,\n",
    "            n_hidden=64,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_extraction_output = F2 * ((((n_times - pool1_stride) // pool1_stride + 1) - pool2_stride) // pool2_stride + 1)\n",
    "\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            FeatureExtraction(n_channels=n_channels, kernel_length=kernel_length, F1=F1, D=D, F2=F2, pool1_stride=pool1_stride, pool2_stride=pool2_stride, dropout_rate=dropout_rate),\n",
    "            nn.Linear(in_features=self.feature_extraction_output, out_features=n_hidden),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "\n",
    "        self.transformer = nn.Sequential(\n",
    "            MyViTransformerWrapper(\n",
    "                image_size = n_times,\n",
    "                patch_size = patches_size,\n",
    "                channels=n_channels,\n",
    "                dropout=dropout_rate,\n",
    "                attn_layers = Encoder(\n",
    "                    dim = transformer_dim,\n",
    "                    depth = transformer_layers,\n",
    "                    heads = transformer_heads,\n",
    "                    macaron=True,\n",
    "                    rel_pos_bias = True\n",
    "                ),\n",
    "            ),\n",
    "            nn.Linear(in_features=transformer_dim, out_features=n_hidden),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            LinearWithConstraint(in_features=2*n_hidden, out_features=n_classes, max_norm=0.25),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        out_values = {}\n",
    "        feature_extraction_result = self.feature_extraction(x)\n",
    "        transformer_result = self.transformer(x)\n",
    "        logits = self.head(torch.cat([\n",
    "            feature_extraction_result,\n",
    "            transformer_result,\n",
    "        ], dim=-1))\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss, out_values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#data augmentation\n",
    "from braindecode.augmentation import FrequencyShift\n",
    "from braindecode.augmentation import GaussianNoise\n",
    "\n",
    "sfreq = epochs.info['sfreq']\n",
    "\n",
    "freq_shift = FrequencyShift(\n",
    "    probability=0.5,  # defines the probability of actually modifying the input\n",
    "    sfreq=sfreq,\n",
    "    max_delta_freq=2.  # the frequency shifts are sampled now between -2 and 2 Hz\n",
    ")\n",
    "\n",
    "gauss_noise = GaussianNoise(\n",
    "    probability=0.5,\n",
    "    std=0.01\n",
    ")\n",
    "\n",
    "transforms = {\n",
    "    'freq': freq_shift,\n",
    "    'gauss': gauss_noise\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = epochs.get_data()\n",
    "X = torch.tensor(data).to(dtype=torch.float32, device=device)\n",
    "y = torch.tensor(labels).to(dtype=torch.long, device=device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = EEGNET(n_times=X.shape[-1], n_channels=len(epochs.picks), n_classes=len(set(labels)))\n",
    "model = model.to(device=device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed = 1330\n",
    "splits = 5\n",
    "lr=3e-4\n",
    "\n",
    "skf = StratifiedKFold(n_splits=splits, random_state=seed, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#fine tune\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from modules.TrainTester import TrainerTester\n",
    "\n",
    "ud = []\n",
    "\n",
    "#main-trianing-loop\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index],\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    TrainerTester.train_loop(model, optimizer, X_train, y_train, X_test, y_test, lr, ud, batch_size=16, iterations=2000)\n",
    "\n",
    "    out_values = TrainerTester.test_loop(model, X_test, y_test)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#params evaluation\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, np.prod(param.size()))\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print('total: ', params)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('model_states/test_model_states.txt'))\n",
    "# torch.save(model.state_dict(), 'model_states/test_model_states.txt')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
