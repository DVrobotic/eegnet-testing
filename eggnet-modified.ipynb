{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import math\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DepthWiseConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, kernel_size, kernels_per_layer, bias=False):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels=in_channels, out_channels=in_channels*kernels_per_layer, kernel_size=kernel_size, groups=in_channels, bias=bias, padding='same')\n",
    "    def forward(self, x):\n",
    "        return self.depthwise(x)\n",
    "\n",
    "class PointWiseConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernels_per_layer=1, bias=False):\n",
    "        super().__init__()\n",
    "        self.pointwise = nn.Conv2d(in_channels=in_channels*kernels_per_layer, out_channels=out_channels,\n",
    "                                   kernel_size=(1,1), bias=bias, padding=\"valid\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pointwise(x)\n",
    "\n",
    "class MaxNormLayer(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, max_norm=1.0,):\n",
    "        super(MaxNormLayer, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.max_norm = max_norm\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.max_norm is not None:\n",
    "            with torch.no_grad():\n",
    "                self.weight.data = torch.renorm(\n",
    "                    self.weight.data, p=2, dim=0, maxnorm=self.max_norm\n",
    "                )\n",
    "        return super(MaxNormLayer, self).forward(x)\n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, kernels_per_layer=1, bias=False):\n",
    "        super().__init__()\n",
    "        self.depthwise = DepthWiseConv2d(in_channels=in_channels, kernels_per_layer=kernels_per_layer, kernel_size=kernel_size, bias=bias)\n",
    "        self.pointwise = PointWiseConv2d(in_channels=in_channels, out_channels=out_channels, kernels_per_layer=kernels_per_layer, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "possíveis melhorias:\n",
    "1 - trocar batchnorm por layernorm\n",
    "2 - aplicação de transformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class EEGNET(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_channels,\n",
    "        n_times,\n",
    "        n_classes,\n",
    "        kernel_length=64,\n",
    "        F1=8,\n",
    "        D=2,\n",
    "        F2=16,\n",
    "        signal_size=32,\n",
    "        pool1_stride=4,\n",
    "        pool2_stride=8,\n",
    "        dropout_rate=0.5,\n",
    "        norm_rate=0.25,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        #block 1\n",
    "        self.conv2d = nn.Conv2d(in_channels=n_channels, out_channels=F1, kernel_size=(1, kernel_length), bias=False, padding='same')\n",
    "        self.batchNorm = nn.BatchNorm2d(num_features=F1, momentum=0.01, eps=0.001, track_running_stats=False)\n",
    "        self.depthWise = DepthWiseConv2d(in_channels=F1, kernel_size=(n_channels, 1), kernels_per_layer=D, bias=False) #equivalente a convolução depth wise\n",
    "        #---------------------------------------------------------------------\n",
    "\n",
    "        #block 2\n",
    "        self.batchNorm2 = nn.BatchNorm2d(num_features=F1*D, momentum=0.01, eps=0.001, track_running_stats=False)\n",
    "        self.elu1 = nn.ELU()\n",
    "        self.avgPool2d = nn.AvgPool2d(kernel_size=(1, pool1_stride), stride=pool1_stride)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.separableConv2d = SeparableConv2d(in_channels=F1*D, kernel_size=(1, 16), out_channels=F2, bias=False)\n",
    "        self.batchNorm3 = nn.BatchNorm2d(num_features=F2, momentum=0.01, eps=0.001, track_running_stats=False)\n",
    "        self.elu2 = nn.ELU()\n",
    "        self.avgPool2d_2 = nn.AvgPool2d(kernel_size=(1, pool2_stride), stride=pool2_stride)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        #---------------------------------------------------------------------\n",
    "\n",
    "        #final block\n",
    "        flatten_output = F2 * ((((n_times - pool1_stride) // pool1_stride + 1) - pool2_stride) // pool2_stride + 1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(flatten_output, signal_size)\n",
    "        self.pos_encoder = PositionalEncoding(signal_size, 0.1)\n",
    "        # self.transformer = nn.TransformerEncoder(\n",
    "        #     nn.TransformerEncoderLayer(d_model=flatten_output, dim_feedforward=flatten_output*2, nhead=1, batch_first=True),\n",
    "        #     num_layers=1\n",
    "        # )\n",
    "        self.maxNormLayer = MaxNormLayer(in_features = signal_size, out_features=n_classes, max_norm=norm_rate)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        #buffers\n",
    "        self.register_buffer('src_mask', EEGNET.generate_src_mask((flatten_output, flatten_output)))\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        out = x.view((x.shape[0], x.shape[1], 1, x.shape[2])) # channel, batch, time (B, C, conv_depth, T)\n",
    "        out = self.conv2d(out) # (B, F1, F1//2, T)\n",
    "        out = self.batchNorm(out) # (B, F1, F1//2, T)\n",
    "        out = self.depthWise(out) # (B, F1*D, F1//2, T)\n",
    "        out = self.batchNorm2(out) # (B, F1*D, F1//2, T)\n",
    "        out = self.elu1(out) # (B, F1*D, F1//2, T)\n",
    "        out = self.avgPool2d(out) # (B, F1*D, max(1, F1//(2*pool1_stride)), max(1, T//(2*pool1_stride)))\n",
    "        out = self.dropout1(out) # (B, F1*D, max(1, F1//(2*pool1_stride)), max(1, T//(2*pool1_stride)))\n",
    "        out = self.separableConv2d(out) # (B, F1*D, max(1, F1//(2*pool1_stride)), max(1, T//(2*pool1_stride)))\n",
    "        out = self.batchNorm3(out) # (B, F1*D, max(1, F1//(2*pool1_stride)), max(1, T//(2*pool1_stride)))\n",
    "        out = self.elu2(out) # (B, F1*D, max(1, F1//(2*pool1_stride)), max(1, T//(2*pool1_stride)))\n",
    "        out = self.avgPool2d_2(out) # (B, F1*D, F1//(2*pool1_stride), T//F2)\n",
    "        out = self.dropout2(out) # (B, F1*D, F1//(2*pool1_stride), T//F2)\n",
    "        out = self.flatten(out) # (B, (F1*D) * (F1//(2*4)) * (T//F2) )\n",
    "        out = self.linear(out)\n",
    "        out = out.view(out.shape[1], out.shape[0], 1)\n",
    "        out = self.pos_encoder(out)\n",
    "        out = out.view(out.shape[1], out.shape[0], out.shape[2])\n",
    "        # out = self.transformer(out)\n",
    "        out = out.mean(2)\n",
    "        out = self.maxNormLayer(out) # (B, n_classes)\n",
    "        logits = self.softmax(out) # (B, n_classes)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_src_mask(src_shape):\n",
    "        \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "        return torch.tril(torch.ones(src_shape), diagonal=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_tgt_mask(tgt_shape):\n",
    "        \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "        return torch.ones(tgt_shape)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = EEGNET(n_channels=22, n_times=1251, n_classes=4)\n",
    "\n",
    "model(torch.randn((1, 22, 1251)), torch.zeros((1,)).to(dtype=torch.long))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "outputs": [],
   "source": [
    "class TrainerTester:\n",
    "\n",
    "    @staticmethod\n",
    "    def train_loop(model, optimizer, X, y, batch_size=32, iterations=1000):\n",
    "        lossi = []\n",
    "        for k in range(iterations):\n",
    "            batch_indexes = torch.randint(0, X.shape[0], (batch_size,))\n",
    "            X_batch, y_batch = X[batch_indexes], y[batch_indexes] # batch X,Y\n",
    "            pred, loss = model(X_batch, y_batch)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #tracking\n",
    "            lossi.append(loss.log10().item())\n",
    "\n",
    "            if k % 100 == 0:\n",
    "                print(f\"loss: {loss} iteration: {k}/{iterations}\")\n",
    "                plt.plot(lossi)\n",
    "                plt.show()\n",
    "\n",
    "        return lossi\n",
    "\n",
    "    @staticmethod\n",
    "    def test_loop(model, Xtest, ytest):\n",
    "        model.eval()\n",
    "        X_batch, y_batch = Xtest, ytest\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred, loss = model(X_batch, y_batch)\n",
    "            correct = (pred.argmax(1) == y_batch).type(torch.float32).sum().item() / y_batch.shape[0]\n",
    "            kappa = cohen_kappa_score(pred.argmax(1), y_batch)\n",
    "\n",
    "        print(f\"Test loss: {loss:>8f} \\n Accuracy: {correct:>8f} \\n kappa: {kappa} \\n\")\n",
    "        model.train()\n",
    "\n",
    "    @staticmethod\n",
    "    def test_and_show(model, Xtest, ytest):\n",
    "        model.eval()\n",
    "        X_batch, y_batch = Xtest, ytest\n",
    "        with torch.no_grad():\n",
    "            pred, loss = model(X_batch, y_batch)\n",
    "            print('shapes: ', pred.shape, y_batch.shape)\n",
    "            accuracy = pred.argmax(1) == y_batch\n",
    "            model.train()\n",
    "            return accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import mne\n",
    "from moabb.datasets import BNCI2014001\n",
    "\n",
    "\n",
    "class BciDataHandler:\n",
    "    def __init__(self):\n",
    "        self.data = BNCI2014001()\n",
    "        self.subjects_epochs = {}\n",
    "        self.subjects_labels = {}\n",
    "        self.subjects_id = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "        self.sessions_id = ['session_T', 'session_E']\n",
    "        self.runs_id = ['run_0', 'run_1', 'run_2', 'run_3', 'run_4', 'run_5']\n",
    "        self.events_desc = {'left_hand': 1, 'right_hand': 2, 'both_feet': 3, 'tongue': 4}\n",
    "        self.tmin, self.tmax, self.t_crop = -.5, 4.5, .5\n",
    "        self.selected_events = ['left_hand', 'right_hand', 'both_feet', 'tongue']\n",
    "        self.raw = self.data.get_data(subjects=self.subjects_id)\n",
    "        self.picks = mne.pick_types(self.raw[1]['session_T']['run_0'].info, eeg=True, stim=False)\n",
    "\n",
    "    def instantiate_dataset(self):\n",
    "        for subject_id in self.subjects_id:\n",
    "            print('subject_id: ', subject_id)\n",
    "            epochs = []\n",
    "            for session_id in self.sessions_id:\n",
    "                print('session_id: ', session_id)\n",
    "                for run_id in self.runs_id:\n",
    "                    loop_raw = self.raw[subject_id][session_id][run_id]\n",
    "                    events = mne.find_events(loop_raw, 'stim')\n",
    "                    run_epochs = mne.Epochs(\n",
    "                        loop_raw,\n",
    "                        events,\n",
    "                        self.events_desc,\n",
    "                        picks=self.picks,\n",
    "                        tmin=self.tmin,\n",
    "                        tmax=self.tmax,\n",
    "                        preload=True\n",
    "                    )[self.selected_events]\n",
    "                    epochs.append(run_epochs)\n",
    "\n",
    "            self.subjects_epochs[subject_id] = (mne.concatenate_epochs(epochs)).filter(5, 60)\n",
    "            self.subjects_labels[subject_id] = [event[2] for event in self.subjects_epochs[subject_id].events]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_handler = BciDataHandler()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_handler.instantiate_dataset()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = EEGNET(n_channels=len(data_handler.picks), n_times=1251, n_classes=len(data_handler.selected_events))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_handler.subjects_epochs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "splits = 5\n",
    "\n",
    "X = torch.tensor(data_handler.subjects_epochs[1].get_data()).to(dtype=torch.float32, device=device)\n",
    "y = torch.tensor(data_handler.subjects_labels[1]).to(dtype=torch.long, device=device) - 1\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = model.to(device=device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#main-trianing-loop\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    TrainerTester.train_loop(model, optimizer, X_train, y_train)\n",
    "    TrainerTester.test_loop(model, X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.439422 \n",
      " Accuracy: 0.250000 \n",
      " kappa: 0.0 \n",
      "\n",
      "shapes:  torch.Size([50, 4]) torch.Size([50])\n",
      "Test loss: 1.378404 \n",
      " Accuracy: 0.347826 \n",
      " kappa: 0.13089480048367608 \n",
      "\n",
      "shapes:  torch.Size([50, 4]) torch.Size([50])\n",
      "Test loss: 1.347256 \n",
      " Accuracy: 0.382609 \n",
      " kappa: 0.17790978654853007 \n",
      "\n",
      "shapes:  torch.Size([50, 4]) torch.Size([50])\n",
      "Test loss: 1.446450 \n",
      " Accuracy: 0.234783 \n",
      " kappa: -0.02170620898536102 \n",
      "\n",
      "shapes:  torch.Size([50, 4]) torch.Size([50])\n",
      "Test loss: 1.386813 \n",
      " Accuracy: 0.330435 \n",
      " kappa: 0.1074488458824715 \n",
      "\n",
      "shapes:  torch.Size([50, 4]) torch.Size([50])\n"
     ]
    },
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x1f780351610>"
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy = []\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    TrainerTester.test_loop(model, X_test, y_test)\n",
    "    accuracy.append(TrainerTester.test_and_show(model, X_test[:50], y_test[:50]).tolist())\n",
    "\n",
    "plt.imshow(accuracy, cmap=\"Blues\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_states/model_states.txt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model_states/model_states.txt'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
