{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meta_step_size = 0.25\n",
    "\n",
    "meta_iters = 1000\n",
    "\n",
    "eval_interval = 1\n",
    "train_shots = 40\n",
    "eval_shots = 4\n",
    "classes = len(set(labels))\n",
    "\n",
    "batch_size = 1\n",
    "#obs: total shots = classes * shots\n",
    "\n",
    "n_times=X.shape[-1]\n",
    "n_channels=len(epochs.picks)\n",
    "\n",
    "seed = 1330\n",
    "splits = 5\n",
    "lr=1e-3\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)\n",
    "\n",
    "train_index, test_index = skf.split(X, y).__next__()\n",
    "X_train, X_test = X[train_index], X[test_index],\n",
    "y_train, y_test = y[train_index], y[test_index]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, training):\n",
    "        split = \"train\" if training else \"test\"\n",
    "\n",
    "        if split:\n",
    "            X_dataset, y_dataset = X_test, y_test\n",
    "        else:\n",
    "            X_dataset, y_dataset = X_train, y_train\n",
    "\n",
    "        self.data = {}\n",
    "\n",
    "        for value, label in zip(X_dataset, y_dataset):\n",
    "            if label not in self.data:\n",
    "                self.data[label] = []\n",
    "            self.data[label].append(value)\n",
    "        self.labels = list(self.data.keys())\n",
    "\n",
    "    def get_mini_dataset(self, shots, num_classes, split=False):\n",
    "        temp_labels = torch.zeros((num_classes * shots))\n",
    "        temp_X = torch.zeros((num_classes * shots, n_channels, n_times))\n",
    "        if split:\n",
    "            test_labels = torch.zeros((num_classes * eval_shots))\n",
    "            test_X = torch.zeros((num_classes * eval_shots, n_channels, n_times))\n",
    "\n",
    "        # Get a random subset of labels from the entire label set.\n",
    "        label_subset = random.choices(self.labels, k=num_classes)\n",
    "        for class_idx, class_obj in enumerate(label_subset):\n",
    "            # Use enumerated index value as a temporary label for mini-batch in\n",
    "            # few shot learning.\n",
    "            temp_labels[class_idx * shots : (class_idx + 1) * shots] = class_idx\n",
    "            # If creating a split dataset for testing, select an extra sample from each\n",
    "            # label to create the test dataset.\n",
    "            if split:\n",
    "                test_labels[class_idx] = class_idx\n",
    "                X_to_split = torch.stack(random.choices(self.data[label_subset[class_idx]], k=shots + 1))\n",
    "                test_X[class_idx] = X_to_split[-1]\n",
    "                temp_X[class_idx * shots : (class_idx + 1) * shots] = X_to_split[:-1]\n",
    "            else:\n",
    "                # For each index in the randomly selected label_subset, sample the\n",
    "                # necessary number of images.\n",
    "                temp_X[class_idx * shots : (class_idx + 1) * shots] = \\\n",
    "                    torch.stack(random.choices(self.data[label_subset[class_idx]], k=shots))\n",
    "\n",
    "        temp_X, temp_labels = unison_shuffled_copies(temp_X, temp_labels)\n",
    "        temp_X, temp_labels = torch.stack(temp_X.chunk(batch_size)), torch.stack(temp_labels.chunk(batch_size))\n",
    "        dataset = zip(temp_X, temp_labels)\n",
    "\n",
    "        if split:\n",
    "            test_X, test_labels = unison_shuffled_copies( test_X, test_labels)\n",
    "            return dataset, test_X, test_labels\n",
    "        return dataset\n",
    "\n",
    "train_dataset = Dataset(training=True)\n",
    "test_dataset = Dataset(training=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training = []\n",
    "testing = []\n",
    "for meta_iter in range(meta_iters):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    frac_done = meta_iter / meta_iters\n",
    "\n",
    "    cur_meta_step_size = (1 - frac_done) * meta_step_size\n",
    "\n",
    "    old_vars = model.state_dict()\n",
    "\n",
    "    mini_dataset = train_dataset.get_mini_dataset(\n",
    "        train_shots, classes\n",
    "    )\n",
    "\n",
    "    for X_values, y_labels in mini_dataset:\n",
    "        y_labels = y_labels.to(dtype=torch.long)\n",
    "        preds, loss, out_values = model(X_values, y_labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    new_vars = model.state_dict()\n",
    "\n",
    "    for key, var in new_vars.items():\n",
    "        new_vars[key] = old_vars[key] + ((new_vars[key] - old_vars[key]) * 0.1)\n",
    "\n",
    "    model.load_state_dict(new_vars)\n",
    "\n",
    "    # Evaluation loop\n",
    "    if meta_iter % eval_interval == 0:\n",
    "        accuracies = []\n",
    "        for dataset in (train_dataset, test_dataset):\n",
    "            # print(\"test dataset reset!\\n\")\n",
    "            # Sample a mini dataset from the full dataset.\n",
    "            train_set, test_X, test_labels = dataset.get_mini_dataset(\n",
    "                eval_shots, classes, split=True\n",
    "            )\n",
    "            old_vars = model.state_dict()\n",
    "\n",
    "            for X_values, y_labels in train_set:\n",
    "                y_labels = y_labels.to(dtype=torch.long)\n",
    "\n",
    "                preds, test_loss, out_values = model(X_values, y_labels)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                test_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            test_labels = test_labels.to(dtype=torch.long)\n",
    "            test_preds, test_loss, test_out_values = model(test_X, test_labels)\n",
    "\n",
    "            accuracy = (test_preds.argmax(1) == test_labels).type(torch.float32).sum().item() / test_labels.shape[0]\n",
    "            accuracies.append(accuracy)\n",
    "\n",
    "            model.load_state_dict(old_vars)\n",
    "\n",
    "        training.append(accuracies[0])\n",
    "        testing.append(accuracies[1])\n",
    "\n",
    "        if meta_iter % 100 == 0:\n",
    "            print(f\"batch {meta_iter}: train={accuracies[0]} test={accuracies[1]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# First, some preprocessing to smooth the training and testing arrays for display.\n",
    "window_length = 100\n",
    "train_s = np.r_[\n",
    "    training[window_length - 1 : 0 : -1], training, training[-1:-window_length:-1]\n",
    "]\n",
    "test_s = np.r_[\n",
    "    testing[window_length - 1 : 0 : -1], testing, testing[-1:-window_length:-1]\n",
    "]\n",
    "w = np.hamming(window_length)\n",
    "train_y = np.convolve(w / w.sum(), train_s, mode=\"valid\")\n",
    "test_y = np.convolve(w / w.sum(), test_s, mode=\"valid\")\n",
    "\n",
    "# Display the training accuracies.\n",
    "x = np.arange(0, len(test_y), 1)\n",
    "plt.plot(x, test_y, x, train_y)\n",
    "plt.legend([\"test\", \"train\"])\n",
    "plt.grid()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
